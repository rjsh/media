<!doctype html>
<html>
<head>
<meta name="viewport" content="width=device-width, initial-scale=1.0"><title>i420_rgb_sse2.h source code [vlc/modules/video_chroma/i420_rgb_sse2.h] - Woboq Code Browser</title>
<link rel="stylesheet" href="../../../../data/qtcreator.css" title="QtCreator"/>
<link rel="alternate stylesheet" href="../../../../data/kdevelop.css" title="KDevelop"/>
<script type="text/javascript" src="../../../../data/jquery/jquery.min.js"></script>
<script type="text/javascript" src="../../../../data/jquery/jquery-ui.min.js"></script>
<script>var file = 'vlc/modules/video_chroma/i420_rgb_sse2.h'; var root_path = '../../..'; var data_path = '../../../../data';</script>
<script src='../../../../data/codebrowser.js'></script>
</head>
<body><div id='header'><h1 id='breadcrumb'><span>Browse the source code of </span><a href='../..'>vlc</a>/<a href='..'>modules</a>/<a href='./'>video_chroma</a>/<a href='i420_rgb_sse2.h.html'>i420_rgb_sse2.h</a></h1></div>
<hr/><div id='content'><table class="code">
<tr><th id="1">1</th><td><i>/*****************************************************************************</i></td></tr>
<tr><th id="2">2</th><td><i> * i420_rgb_sse2.h: MMX YUV transformation assembly</i></td></tr>
<tr><th id="3">3</th><td><i> *****************************************************************************</i></td></tr>
<tr><th id="4">4</th><td><i> * Copyright (C) 1999-2012 VLC authors and VideoLAN</i></td></tr>
<tr><th id="5">5</th><td><i> *</i></td></tr>
<tr><th id="6">6</th><td><i> * Authors: Damien Fouilleul &lt;damienf@videolan.org&gt;</i></td></tr>
<tr><th id="7">7</th><td><i> *</i></td></tr>
<tr><th id="8">8</th><td><i> * This program is free software; you can redistribute it and/or modify it</i></td></tr>
<tr><th id="9">9</th><td><i> * under the terms of the GNU Lesser General Public License as published by</i></td></tr>
<tr><th id="10">10</th><td><i> * the Free Software Foundation; either version 2.1 of the License, or</i></td></tr>
<tr><th id="11">11</th><td><i> * (at your option) any later version.</i></td></tr>
<tr><th id="12">12</th><td><i> *</i></td></tr>
<tr><th id="13">13</th><td><i> * This program is distributed in the hope that it will be useful,</i></td></tr>
<tr><th id="14">14</th><td><i> * but WITHOUT ANY WARRANTY; without even the implied warranty of</i></td></tr>
<tr><th id="15">15</th><td><i> * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the</i></td></tr>
<tr><th id="16">16</th><td><i> * GNU Lesser General Public License for more details.</i></td></tr>
<tr><th id="17">17</th><td><i> *</i></td></tr>
<tr><th id="18">18</th><td><i> * You should have received a copy of the GNU Lesser General Public License</i></td></tr>
<tr><th id="19">19</th><td><i> * along with this program; if not, write to the Free Software Foundation,</i></td></tr>
<tr><th id="20">20</th><td><i> * Inc., 51 Franklin Street, Fifth Floor, Boston MA 02110-1301, USA.</i></td></tr>
<tr><th id="21">21</th><td><i> *****************************************************************************/</i></td></tr>
<tr><th id="22">22</th><td><u>#<span data-ppcond="22">if</span> defined(<a class="macro" href="../../config.h.html#38" data-ref="_M/CAN_COMPILE_SSE2">CAN_COMPILE_SSE2</a>)</u></td></tr>
<tr><th id="23">23</th><td></td></tr>
<tr><th id="24">24</th><td><i>/* SSE2 assembly */</i></td></tr>
<tr><th id="25">25</th><td></td></tr>
<tr><th id="26">26</th><td><u>#define <dfn class="macro" id="_M/SSE2_CALL" data-ref="_M/SSE2_CALL">SSE2_CALL</dfn>(SSE2_INSTRUCTIONS)    \</u></td></tr>
<tr><th id="27">27</th><td><u>    do {                                \</u></td></tr>
<tr><th id="28">28</th><td><u>    __asm__ __volatile__(               \</u></td></tr>
<tr><th id="29">29</th><td><u>        ".p2align 3 \n\t"               \</u></td></tr>
<tr><th id="30">30</th><td><u>        SSE2_INSTRUCTIONS               \</u></td></tr>
<tr><th id="31">31</th><td><u>        :                               \</u></td></tr>
<tr><th id="32">32</th><td><u>        : "r" (p_y), "r" (p_u),         \</u></td></tr>
<tr><th id="33">33</th><td><u>          "r" (p_v), "r" (p_buffer)     \</u></td></tr>
<tr><th id="34">34</th><td><u>        : "eax", "xmm0", "xmm1", "xmm2", "xmm3", \</u></td></tr>
<tr><th id="35">35</th><td><u>                 "xmm4", "xmm5", "xmm6", "xmm7" ); \</u></td></tr>
<tr><th id="36">36</th><td><u>    } while(0)</u></td></tr>
<tr><th id="37">37</th><td></td></tr>
<tr><th id="38">38</th><td><u>#define <dfn class="macro" id="_M/SSE2_END" data-ref="_M/SSE2_END">SSE2_END</dfn>  __asm__ __volatile__ ( "sfence" ::: "memory" )</u></td></tr>
<tr><th id="39">39</th><td></td></tr>
<tr><th id="40">40</th><td><u>#define <dfn class="macro" id="_M/SSE2_INIT_16_ALIGNED" data-ref="_M/SSE2_INIT_16_ALIGNED">SSE2_INIT_16_ALIGNED</dfn> "                                              \n\</u></td></tr>
<tr><th id="41">41</th><td><u>movq        (%1), %%xmm0    # Load 8 Cb       00 00 00 00 u3 u2 u1 u0       \n\</u></td></tr>
<tr><th id="42">42</th><td><u>movq        (%2), %%xmm1    # Load 8 Cr       00 00 00 00 v3 v2 v1 v0       \n\</u></td></tr>
<tr><th id="43">43</th><td><u>pxor      %%xmm4, %%xmm4    # zero mm4                                      \n\</u></td></tr>
<tr><th id="44">44</th><td><u>movdqa      (%0), %%xmm6    # Load 16 Y       Y7 Y6 Y5 Y4 Y3 Y2 Y1 Y0       \n\</u></td></tr>
<tr><th id="45">45</th><td><u>"</u></td></tr>
<tr><th id="46">46</th><td></td></tr>
<tr><th id="47">47</th><td><u>#define <dfn class="macro" id="_M/SSE2_INIT_16_UNALIGNED" data-ref="_M/SSE2_INIT_16_UNALIGNED">SSE2_INIT_16_UNALIGNED</dfn> "                                            \n\</u></td></tr>
<tr><th id="48">48</th><td><u>movq        (%1), %%xmm0    # Load 8 Cb       00 00 00 00 u3 u2 u1 u0       \n\</u></td></tr>
<tr><th id="49">49</th><td><u>movq        (%2), %%xmm1    # Load 8 Cr       00 00 00 00 v3 v2 v1 v0       \n\</u></td></tr>
<tr><th id="50">50</th><td><u>pxor      %%xmm4, %%xmm4    # zero mm4                                      \n\</u></td></tr>
<tr><th id="51">51</th><td><u>movdqu      (%0), %%xmm6    # Load 16 Y       Y7 Y6 Y5 Y4 Y3 Y2 Y1 Y0       \n\</u></td></tr>
<tr><th id="52">52</th><td><u>prefetchnta (%3)            # Tell CPU not to cache output RGB data         \n\</u></td></tr>
<tr><th id="53">53</th><td><u>"</u></td></tr>
<tr><th id="54">54</th><td></td></tr>
<tr><th id="55">55</th><td><u>#define <dfn class="macro" id="_M/SSE2_INIT_32_ALIGNED" data-ref="_M/SSE2_INIT_32_ALIGNED">SSE2_INIT_32_ALIGNED</dfn> "                                              \n\</u></td></tr>
<tr><th id="56">56</th><td><u>movq        (%1), %%xmm0    # Load 8 Cb       00 00 00 00 u3 u2 u1 u0       \n\</u></td></tr>
<tr><th id="57">57</th><td><u>movq        (%2), %%xmm1    # Load 8 Cr       00 00 00 00 v3 v2 v1 v0       \n\</u></td></tr>
<tr><th id="58">58</th><td><u>pxor      %%xmm4, %%xmm4    # zero mm4                                      \n\</u></td></tr>
<tr><th id="59">59</th><td><u>movdqa      (%0), %%xmm6    # Load 16 Y       Y7 Y6 Y5 Y4 Y3 Y2 Y1 Y0       \n\</u></td></tr>
<tr><th id="60">60</th><td><u>"</u></td></tr>
<tr><th id="61">61</th><td></td></tr>
<tr><th id="62">62</th><td><u>#define <dfn class="macro" id="_M/SSE2_INIT_32_UNALIGNED" data-ref="_M/SSE2_INIT_32_UNALIGNED">SSE2_INIT_32_UNALIGNED</dfn> "                                            \n\</u></td></tr>
<tr><th id="63">63</th><td><u>movq        (%1), %%xmm0    # Load 8 Cb       00 00 00 00 u3 u2 u1 u0       \n\</u></td></tr>
<tr><th id="64">64</th><td><u>movq        (%2), %%xmm1    # Load 8 Cr       00 00 00 00 v3 v2 v1 v0       \n\</u></td></tr>
<tr><th id="65">65</th><td><u>pxor      %%xmm4, %%xmm4    # zero mm4                                      \n\</u></td></tr>
<tr><th id="66">66</th><td><u>movdqu      (%0), %%xmm6    # Load 16 Y       Y7 Y6 Y5 Y4 Y3 Y2 Y1 Y0       \n\</u></td></tr>
<tr><th id="67">67</th><td><u>prefetchnta (%3)            # Tell CPU not to cache output RGB data         \n\</u></td></tr>
<tr><th id="68">68</th><td><u>"</u></td></tr>
<tr><th id="69">69</th><td></td></tr>
<tr><th id="70">70</th><td><u>#define <dfn class="macro" id="_M/SSE2_YUV_MUL" data-ref="_M/SSE2_YUV_MUL">SSE2_YUV_MUL</dfn> "                                                      \n\</u></td></tr>
<tr><th id="71">71</th><td><u># convert the chroma part                                                   \n\</u></td></tr>
<tr><th id="72">72</th><td><u>punpcklbw %%xmm4, %%xmm0        # scatter 8 Cb    00 u3 00 u2 00 u1 00 u0   \n\</u></td></tr>
<tr><th id="73">73</th><td><u>punpcklbw %%xmm4, %%xmm1        # scatter 8 Cr    00 v3 00 v2 00 v1 00 v0   \n\</u></td></tr>
<tr><th id="74">74</th><td><u>movl      $0x00800080, %%eax    #                                           \n\</u></td></tr>
<tr><th id="75">75</th><td><u>movd      %%eax, %%xmm5         #                                           \n\</u></td></tr>
<tr><th id="76">76</th><td><u>pshufd    $0, %%xmm5, %%xmm5    # Set xmm5 to     0080 0080 ... 0080 0080   \n\</u></td></tr>
<tr><th id="77">77</th><td><u>psubsw    %%xmm5, %%xmm0        # Cb -= 128                                 \n\</u></td></tr>
<tr><th id="78">78</th><td><u>psubsw    %%xmm5, %%xmm1        # Cr -= 128                                 \n\</u></td></tr>
<tr><th id="79">79</th><td><u>psllw     $3, %%xmm0            # Promote precision                         \n\</u></td></tr>
<tr><th id="80">80</th><td><u>psllw     $3, %%xmm1            # Promote precision                         \n\</u></td></tr>
<tr><th id="81">81</th><td><u>movdqa    %%xmm0, %%xmm2        # Copy 8 Cb       00 u3 00 u2 00 u1 00 u0   \n\</u></td></tr>
<tr><th id="82">82</th><td><u>movdqa    %%xmm1, %%xmm3        # Copy 8 Cr       00 v3 00 v2 00 v1 00 v0   \n\</u></td></tr>
<tr><th id="83">83</th><td><u>movl      $0xf37df37d, %%eax    #                                           \n\</u></td></tr>
<tr><th id="84">84</th><td><u>movd      %%eax, %%xmm5         #                                           \n\</u></td></tr>
<tr><th id="85">85</th><td><u>pshufd    $0, %%xmm5, %%xmm5    # Set xmm5 to     f37d f37d ... f37d f37d   \n\</u></td></tr>
<tr><th id="86">86</th><td><u>pmulhw    %%xmm5, %%xmm2        # Mul Cb with green coeff -&gt; Cb green       \n\</u></td></tr>
<tr><th id="87">87</th><td><u>movl      $0xe5fce5fc, %%eax    #                                           \n\</u></td></tr>
<tr><th id="88">88</th><td><u>movd      %%eax, %%xmm5         #                                           \n\</u></td></tr>
<tr><th id="89">89</th><td><u>pshufd    $0, %%xmm5, %%xmm5    # Set xmm5 to     e5fc e5fc ... e5fc e5fc   \n\</u></td></tr>
<tr><th id="90">90</th><td><u>pmulhw    %%xmm5, %%xmm3        # Mul Cr with green coeff -&gt; Cr green       \n\</u></td></tr>
<tr><th id="91">91</th><td><u>movl      $0x40934093, %%eax    #                                           \n\</u></td></tr>
<tr><th id="92">92</th><td><u>movd      %%eax, %%xmm5         #                                           \n\</u></td></tr>
<tr><th id="93">93</th><td><u>pshufd    $0, %%xmm5, %%xmm5    # Set xmm5 to     4093 4093 ... 4093 4093   \n\</u></td></tr>
<tr><th id="94">94</th><td><u>pmulhw    %%xmm5, %%xmm0        # Mul Cb -&gt; Cblue 00 b3 00 b2 00 b1 00 b0   \n\</u></td></tr>
<tr><th id="95">95</th><td><u>movl      $0x33123312, %%eax    #                                           \n\</u></td></tr>
<tr><th id="96">96</th><td><u>movd      %%eax, %%xmm5         #                                           \n\</u></td></tr>
<tr><th id="97">97</th><td><u>pshufd    $0, %%xmm5, %%xmm5    # Set xmm5 to     3312 3312 ... 3312 3312   \n\</u></td></tr>
<tr><th id="98">98</th><td><u>pmulhw    %%xmm5, %%xmm1        # Mul Cr -&gt; Cred  00 r3 00 r2 00 r1 00 r0   \n\</u></td></tr>
<tr><th id="99">99</th><td><u>paddsw    %%xmm3, %%xmm2        # Cb green + Cr green -&gt; Cgreen             \n\</u></td></tr>
<tr><th id="100">100</th><td><u>                                                                            \n\</u></td></tr>
<tr><th id="101">101</th><td><u># convert the luma part                                                     \n\</u></td></tr>
<tr><th id="102">102</th><td><u>movl      $0x10101010, %%eax    #                                           \n\</u></td></tr>
<tr><th id="103">103</th><td><u>movd      %%eax, %%xmm5         #                                           \n\</u></td></tr>
<tr><th id="104">104</th><td><u>pshufd    $0, %%xmm5, %%xmm5    # Set xmm5 to   1010 1010 ... 1010 1010     \n\</u></td></tr>
<tr><th id="105">105</th><td><u>psubusb   %%xmm5, %%xmm6        # Y -= 16                                   \n\</u></td></tr>
<tr><th id="106">106</th><td><u>movdqa    %%xmm6, %%xmm7        # Copy 16 Y       Y7 Y6 Y5 Y4 Y3 Y2 Y1 Y0   \n\</u></td></tr>
<tr><th id="107">107</th><td><u>movl      $0x00ff00ff, %%eax    #                                           \n\</u></td></tr>
<tr><th id="108">108</th><td><u>movd      %%eax, %%xmm5         #                                           \n\</u></td></tr>
<tr><th id="109">109</th><td><u>pshufd    $0, %%xmm5, %%xmm5    # set xmm5 to     00ff 00ff ... 00ff 00ff   \n\</u></td></tr>
<tr><th id="110">110</th><td><u>pand      %%xmm5, %%xmm6        # get Y even      00 Y6 00 Y4 00 Y2 00 Y0   \n\</u></td></tr>
<tr><th id="111">111</th><td><u>psrlw     $8, %%xmm7            # get Y odd       00 Y7 00 Y5 00 Y3 00 Y1   \n\</u></td></tr>
<tr><th id="112">112</th><td><u>psllw     $3, %%xmm6            # Promote precision                         \n\</u></td></tr>
<tr><th id="113">113</th><td><u>psllw     $3, %%xmm7            # Promote precision                         \n\</u></td></tr>
<tr><th id="114">114</th><td><u>movl      $0x253f253f, %%eax    #                                           \n\</u></td></tr>
<tr><th id="115">115</th><td><u>movd      %%eax, %%xmm5         #                                           \n\</u></td></tr>
<tr><th id="116">116</th><td><u>pshufd    $0, %%xmm5, %%xmm5    # set xmm5 to     253f 253f ... 253f 253f   \n\</u></td></tr>
<tr><th id="117">117</th><td><u>pmulhw    %%xmm5, %%xmm6        # Mul 8 Y even    00 y6 00 y4 00 y2 00 y0   \n\</u></td></tr>
<tr><th id="118">118</th><td><u>pmulhw    %%xmm5, %%xmm7        # Mul 8 Y odd     00 y7 00 y5 00 y3 00 y1   \n\</u></td></tr>
<tr><th id="119">119</th><td><u>"</u></td></tr>
<tr><th id="120">120</th><td></td></tr>
<tr><th id="121">121</th><td><u>#define <dfn class="macro" id="_M/SSE2_YUV_ADD" data-ref="_M/SSE2_YUV_ADD">SSE2_YUV_ADD</dfn> "                                                      \n\</u></td></tr>
<tr><th id="122">122</th><td><u># Do horizontal and vertical scaling                                        \n\</u></td></tr>
<tr><th id="123">123</th><td><u>movdqa    %%xmm0, %%xmm3        # Copy Cblue                                \n\</u></td></tr>
<tr><th id="124">124</th><td><u>movdqa    %%xmm1, %%xmm4        # Copy Cred                                 \n\</u></td></tr>
<tr><th id="125">125</th><td><u>movdqa    %%xmm2, %%xmm5        # Copy Cgreen                               \n\</u></td></tr>
<tr><th id="126">126</th><td><u>paddsw    %%xmm6, %%xmm0        # Y even + Cblue  00 B6 00 B4 00 B2 00 B0   \n\</u></td></tr>
<tr><th id="127">127</th><td><u>paddsw    %%xmm7, %%xmm3        # Y odd  + Cblue  00 B7 00 B5 00 B3 00 B1   \n\</u></td></tr>
<tr><th id="128">128</th><td><u>paddsw    %%xmm6, %%xmm1        # Y even + Cred   00 R6 00 R4 00 R2 00 R0   \n\</u></td></tr>
<tr><th id="129">129</th><td><u>paddsw    %%xmm7, %%xmm4        # Y odd  + Cred   00 R7 00 R5 00 R3 00 R1   \n\</u></td></tr>
<tr><th id="130">130</th><td><u>paddsw    %%xmm6, %%xmm2        # Y even + Cgreen 00 G6 00 G4 00 G2 00 G0   \n\</u></td></tr>
<tr><th id="131">131</th><td><u>paddsw    %%xmm7, %%xmm5        # Y odd  + Cgreen 00 G7 00 G5 00 G3 00 G1   \n\</u></td></tr>
<tr><th id="132">132</th><td><u>                                                                            \n\</u></td></tr>
<tr><th id="133">133</th><td><u># Limit RGB even to 0..255                                                  \n\</u></td></tr>
<tr><th id="134">134</th><td><u>packuswb  %%xmm0, %%xmm0        # B6 B4 B2 B0 / B6 B4 B2 B0                 \n\</u></td></tr>
<tr><th id="135">135</th><td><u>packuswb  %%xmm1, %%xmm1        # R6 R4 R2 R0 / R6 R4 R2 R0                 \n\</u></td></tr>
<tr><th id="136">136</th><td><u>packuswb  %%xmm2, %%xmm2        # G6 G4 G2 G0 / G6 G4 G2 G0                 \n\</u></td></tr>
<tr><th id="137">137</th><td><u>                                                                            \n\</u></td></tr>
<tr><th id="138">138</th><td><u># Limit RGB odd to 0..255                                                   \n\</u></td></tr>
<tr><th id="139">139</th><td><u>packuswb  %%xmm3, %%xmm3        # B7 B5 B3 B1 / B7 B5 B3 B1                 \n\</u></td></tr>
<tr><th id="140">140</th><td><u>packuswb  %%xmm4, %%xmm4        # R7 R5 R3 R1 / R7 R5 R3 R1                 \n\</u></td></tr>
<tr><th id="141">141</th><td><u>packuswb  %%xmm5, %%xmm5        # G7 G5 G3 G1 / G7 G5 G3 G1                 \n\</u></td></tr>
<tr><th id="142">142</th><td><u>                                                                            \n\</u></td></tr>
<tr><th id="143">143</th><td><u># Interleave RGB even and odd                                               \n\</u></td></tr>
<tr><th id="144">144</th><td><u>punpcklbw %%xmm3, %%xmm0        #                 B7 B6 B5 B4 B3 B2 B1 B0   \n\</u></td></tr>
<tr><th id="145">145</th><td><u>punpcklbw %%xmm4, %%xmm1        #                 R7 R6 R5 R4 R3 R2 R1 R0   \n\</u></td></tr>
<tr><th id="146">146</th><td><u>punpcklbw %%xmm5, %%xmm2        #                 G7 G6 G5 G4 G3 G2 G1 G0   \n\</u></td></tr>
<tr><th id="147">147</th><td><u>"</u></td></tr>
<tr><th id="148">148</th><td></td></tr>
<tr><th id="149">149</th><td><u>#define <dfn class="macro" id="_M/SSE2_UNPACK_15_ALIGNED" data-ref="_M/SSE2_UNPACK_15_ALIGNED">SSE2_UNPACK_15_ALIGNED</dfn> "                                            \n\</u></td></tr>
<tr><th id="150">150</th><td><u># mask unneeded bits off                                                    \n\</u></td></tr>
<tr><th id="151">151</th><td><u>movl      $0xf8f8f8f8, %%eax    #                                           \n\</u></td></tr>
<tr><th id="152">152</th><td><u>movd      %%eax, %%xmm5         #                                           \n\</u></td></tr>
<tr><th id="153">153</th><td><u>pshufd    $0, %%xmm5, %%xmm5    # set xmm5 to     f8f8 f8f8 ... f8f8 f8f8   \n\</u></td></tr>
<tr><th id="154">154</th><td><u>pand      %%xmm5, %%xmm0        # b7b6b5b4 b3______ b7b6b5b4 b3______       \n\</u></td></tr>
<tr><th id="155">155</th><td><u>psrlw     $3,%%xmm0             # ______b7 b6b5b4b3 ______b7 b6b5b4b3       \n\</u></td></tr>
<tr><th id="156">156</th><td><u>pand      %%xmm5, %%xmm2        # g7g6g5g4 g3______ g7g6g5g4 g3______       \n\</u></td></tr>
<tr><th id="157">157</th><td><u>pand      %%xmm5, %%xmm1        # r7r6r5r4 r3______ r7r6r5r4 r3______       \n\</u></td></tr>
<tr><th id="158">158</th><td><u>psrlw     $1,%%xmm1             # __r7r6r5 r4r3____ __r7r6r5 r4r3____       \n\</u></td></tr>
<tr><th id="159">159</th><td><u>pxor      %%xmm4, %%xmm4        # zero mm4                                  \n\</u></td></tr>
<tr><th id="160">160</th><td><u>movdqa    %%xmm0, %%xmm5        # Copy B15-B0                               \n\</u></td></tr>
<tr><th id="161">161</th><td><u>movdqa    %%xmm2, %%xmm7        # Copy G15-G0                               \n\</u></td></tr>
<tr><th id="162">162</th><td><u>                                                                            \n\</u></td></tr>
<tr><th id="163">163</th><td><u># convert rgb24 plane to rgb15 pack for pixel 0-7                           \n\</u></td></tr>
<tr><th id="164">164</th><td><u>punpcklbw %%xmm4, %%xmm2        # ________ ________ g7g6g5g4 g3______       \n\</u></td></tr>
<tr><th id="165">165</th><td><u>punpcklbw %%xmm1, %%xmm0        # r7r6r5r4 r3______ ______b7 b6b5b4b3       \n\</u></td></tr>
<tr><th id="166">166</th><td><u>psllw     $2,%%xmm2             # ________ ____g7g6 g5g4g3__ ________       \n\</u></td></tr>
<tr><th id="167">167</th><td><u>por       %%xmm2, %%xmm0        # r7r6r5r4 r3__g7g6 g5g4g3b7 b6b5b4b3       \n\</u></td></tr>
<tr><th id="168">168</th><td><u>movntdq   %%xmm0, (%3)          # store pixel 0-7                           \n\</u></td></tr>
<tr><th id="169">169</th><td><u>                                                                            \n\</u></td></tr>
<tr><th id="170">170</th><td><u># convert rgb24 plane to rgb15 pack for pixel 8-15                          \n\</u></td></tr>
<tr><th id="171">171</th><td><u>punpckhbw %%xmm4, %%xmm7        # ________ ________ g7g6g5g4 g3______       \n\</u></td></tr>
<tr><th id="172">172</th><td><u>punpckhbw %%xmm1, %%xmm5        # r7r6r5r4 r3______ ______b7 b6b5b4b3       \n\</u></td></tr>
<tr><th id="173">173</th><td><u>psllw     $2,%%xmm7             # ________ ____g7g6 g5g4g3__ ________       \n\</u></td></tr>
<tr><th id="174">174</th><td><u>por       %%xmm7, %%xmm5        # r7r6r5r4 r3__g7g6 g5g4g3b7 b6b5b4b3       \n\</u></td></tr>
<tr><th id="175">175</th><td><u>movntdq   %%xmm5, 16(%3)        # store pixel 4-7                           \n\</u></td></tr>
<tr><th id="176">176</th><td><u>"</u></td></tr>
<tr><th id="177">177</th><td></td></tr>
<tr><th id="178">178</th><td><u>#define <dfn class="macro" id="_M/SSE2_UNPACK_15_UNALIGNED" data-ref="_M/SSE2_UNPACK_15_UNALIGNED">SSE2_UNPACK_15_UNALIGNED</dfn> "                                          \n\</u></td></tr>
<tr><th id="179">179</th><td><u># mask unneeded bits off                                                    \n\</u></td></tr>
<tr><th id="180">180</th><td><u>movl      $0xf8f8f8f8, %%eax    #                                           \n\</u></td></tr>
<tr><th id="181">181</th><td><u>movd      %%eax, %%xmm5         #                                           \n\</u></td></tr>
<tr><th id="182">182</th><td><u>pshufd    $0, %%xmm5, %%xmm5    # set xmm5 to     f8f8 f8f8 ... f8f8 f8f8   \n\</u></td></tr>
<tr><th id="183">183</th><td><u>pand      %%xmm5, %%xmm0        # b7b6b5b4 b3______ b7b6b5b4 b3______       \n\</u></td></tr>
<tr><th id="184">184</th><td><u>psrlw     $3,%%xmm0             # ______b7 b6b5b4b3 ______b7 b6b5b4b3       \n\</u></td></tr>
<tr><th id="185">185</th><td><u>pand      %%xmm5, %%xmm2        # g7g6g5g4 g3______ g7g6g5g4 g3______       \n\</u></td></tr>
<tr><th id="186">186</th><td><u>pand      %%xmm5, %%xmm1        # r7r6r5r4 r3______ r7r6r5r4 r3______       \n\</u></td></tr>
<tr><th id="187">187</th><td><u>psrlw     $1,%%xmm1             # __r7r6r5 r4r3____ __r7r6r5 r4r3____       \n\</u></td></tr>
<tr><th id="188">188</th><td><u>pxor      %%xmm4, %%xmm4        # zero mm4                                  \n\</u></td></tr>
<tr><th id="189">189</th><td><u>movdqa    %%xmm0, %%xmm5        # Copy B15-B0                               \n\</u></td></tr>
<tr><th id="190">190</th><td><u>movdqa    %%xmm2, %%xmm7        # Copy G15-G0                               \n\</u></td></tr>
<tr><th id="191">191</th><td><u>                                                                            \n\</u></td></tr>
<tr><th id="192">192</th><td><u># convert rgb24 plane to rgb15 pack for pixel 0-7                           \n\</u></td></tr>
<tr><th id="193">193</th><td><u>punpcklbw %%xmm4, %%xmm2        # ________ ________ g7g6g5g4 g3______       \n\</u></td></tr>
<tr><th id="194">194</th><td><u>punpcklbw %%xmm1, %%xmm0        # r7r6r5r4 r3______ ______b7 b6b5b4b3       \n\</u></td></tr>
<tr><th id="195">195</th><td><u>psllw     $2,%%xmm2             # ________ ____g7g6 g5g4g3__ ________       \n\</u></td></tr>
<tr><th id="196">196</th><td><u>por       %%xmm2, %%xmm0        # r7r6r5r4 r3__g7g6 g5g4g3b7 b6b5b4b3       \n\</u></td></tr>
<tr><th id="197">197</th><td><u>movdqu    %%xmm0, (%3)          # store pixel 0-7                           \n\</u></td></tr>
<tr><th id="198">198</th><td><u>                                                                            \n\</u></td></tr>
<tr><th id="199">199</th><td><u># convert rgb24 plane to rgb15 pack for pixel 8-15                          \n\</u></td></tr>
<tr><th id="200">200</th><td><u>punpckhbw %%xmm4, %%xmm7        # ________ ________ g7g6g5g4 g3______       \n\</u></td></tr>
<tr><th id="201">201</th><td><u>punpckhbw %%xmm1, %%xmm5        # r7r6r5r4 r3______ ______b7 b6b5b4b3       \n\</u></td></tr>
<tr><th id="202">202</th><td><u>psllw     $2,%%xmm7             # ________ ____g7g6 g5g4g3__ ________       \n\</u></td></tr>
<tr><th id="203">203</th><td><u>por       %%xmm7, %%xmm5        # r7r6r5r4 r3__g7g6 g5g4g3b7 b6b5b4b3       \n\</u></td></tr>
<tr><th id="204">204</th><td><u>movdqu    %%xmm5, 16(%3)        # store pixel 4-7                           \n\</u></td></tr>
<tr><th id="205">205</th><td><u>"</u></td></tr>
<tr><th id="206">206</th><td></td></tr>
<tr><th id="207">207</th><td><u>#define <dfn class="macro" id="_M/SSE2_UNPACK_16_ALIGNED" data-ref="_M/SSE2_UNPACK_16_ALIGNED">SSE2_UNPACK_16_ALIGNED</dfn> "                                            \n\</u></td></tr>
<tr><th id="208">208</th><td><u># mask unneeded bits off                                                    \n\</u></td></tr>
<tr><th id="209">209</th><td><u>movl      $0xf8f8f8f8, %%eax    #                                           \n\</u></td></tr>
<tr><th id="210">210</th><td><u>movd      %%eax, %%xmm5         #                                           \n\</u></td></tr>
<tr><th id="211">211</th><td><u>pshufd    $0, %%xmm5, %%xmm5    # set xmm5 to     f8f8 f8f8 ... f8f8 f8f8   \n\</u></td></tr>
<tr><th id="212">212</th><td><u>pand      %%xmm5, %%xmm0        # b7b6b5b4 b3______ b7b6b5b4 b3______       \n\</u></td></tr>
<tr><th id="213">213</th><td><u>pand      %%xmm5, %%xmm1        # r7r6r5r4 r3______ r7r6r5r4 r3______       \n\</u></td></tr>
<tr><th id="214">214</th><td><u>movl      $0xfcfcfcfc, %%eax    #                                           \n\</u></td></tr>
<tr><th id="215">215</th><td><u>movd      %%eax, %%xmm5         #                                           \n\</u></td></tr>
<tr><th id="216">216</th><td><u>pshufd    $0, %%xmm5, %%xmm5    # set xmm5 to     f8f8 f8f8 ... f8f8 f8f8   \n\</u></td></tr>
<tr><th id="217">217</th><td><u>pand      %%xmm5, %%xmm2        # g7g6g5g4 g3g2____ g7g6g5g4 g3g2____       \n\</u></td></tr>
<tr><th id="218">218</th><td><u>psrlw     $3,%%xmm0             # ______b7 b6b5b4b3 ______b7 b6b5b4b3       \n\</u></td></tr>
<tr><th id="219">219</th><td><u>pxor      %%xmm4, %%xmm4        # zero mm4                                  \n\</u></td></tr>
<tr><th id="220">220</th><td><u>movdqa    %%xmm0, %%xmm5        # Copy B15-B0                               \n\</u></td></tr>
<tr><th id="221">221</th><td><u>movdqa    %%xmm2, %%xmm7        # Copy G15-G0                               \n\</u></td></tr>
<tr><th id="222">222</th><td><u>                                                                            \n\</u></td></tr>
<tr><th id="223">223</th><td><u># convert rgb24 plane to rgb16 pack for pixel 0-7                           \n\</u></td></tr>
<tr><th id="224">224</th><td><u>punpcklbw %%xmm4, %%xmm2        # ________ ________ g7g6g5g4 g3g2____       \n\</u></td></tr>
<tr><th id="225">225</th><td><u>punpcklbw %%xmm1, %%xmm0        # r7r6r5r4 r3______ ______b7 b6b5b4b3       \n\</u></td></tr>
<tr><th id="226">226</th><td><u>psllw     $3,%%xmm2             # ________ __g7g6g5 g4g3g2__ ________       \n\</u></td></tr>
<tr><th id="227">227</th><td><u>por       %%xmm2, %%xmm0        # r7r6r5r4 r3g7g6g5 g4g3g2b7 b6b5b4b3       \n\</u></td></tr>
<tr><th id="228">228</th><td><u>movntdq   %%xmm0, (%3)          # store pixel 0-7                           \n\</u></td></tr>
<tr><th id="229">229</th><td><u>                                                                            \n\</u></td></tr>
<tr><th id="230">230</th><td><u># convert rgb24 plane to rgb16 pack for pixel 8-15                          \n\</u></td></tr>
<tr><th id="231">231</th><td><u>punpckhbw %%xmm4, %%xmm7        # ________ ________ g7g6g5g4 g3g2____       \n\</u></td></tr>
<tr><th id="232">232</th><td><u>punpckhbw %%xmm1, %%xmm5        # r7r6r5r4 r3______ ______b7 b6b5b4b3       \n\</u></td></tr>
<tr><th id="233">233</th><td><u>psllw     $3,%%xmm7             # ________ __g7g6g5 g4g3g2__ ________       \n\</u></td></tr>
<tr><th id="234">234</th><td><u>por       %%xmm7, %%xmm5        # r7r6r5r4 r3g7g6g5 g4g3g2b7 b6b5b4b3       \n\</u></td></tr>
<tr><th id="235">235</th><td><u>movntdq   %%xmm5, 16(%3)        # store pixel 4-7                           \n\</u></td></tr>
<tr><th id="236">236</th><td><u>"</u></td></tr>
<tr><th id="237">237</th><td></td></tr>
<tr><th id="238">238</th><td><u>#define <dfn class="macro" id="_M/SSE2_UNPACK_16_UNALIGNED" data-ref="_M/SSE2_UNPACK_16_UNALIGNED">SSE2_UNPACK_16_UNALIGNED</dfn> "                                          \n\</u></td></tr>
<tr><th id="239">239</th><td><u># mask unneeded bits off                                                    \n\</u></td></tr>
<tr><th id="240">240</th><td><u>movl      $0xf8f8f8f8, %%eax    #                                           \n\</u></td></tr>
<tr><th id="241">241</th><td><u>movd      %%eax, %%xmm5         #                                           \n\</u></td></tr>
<tr><th id="242">242</th><td><u>pshufd    $0, %%xmm5, %%xmm5    # set xmm5 to     f8f8 f8f8 ... f8f8 f8f8   \n\</u></td></tr>
<tr><th id="243">243</th><td><u>pand      %%xmm5, %%xmm0        # b7b6b5b4 b3______ b7b6b5b4 b3______       \n\</u></td></tr>
<tr><th id="244">244</th><td><u>pand      %%xmm5, %%xmm1        # r7r6r5r4 r3______ r7r6r5r4 r3______       \n\</u></td></tr>
<tr><th id="245">245</th><td><u>movl      $0xfcfcfcfc, %%eax    #                                           \n\</u></td></tr>
<tr><th id="246">246</th><td><u>movd      %%eax, %%xmm5         #                                           \n\</u></td></tr>
<tr><th id="247">247</th><td><u>pshufd    $0, %%xmm5, %%xmm5    # set xmm5 to     f8f8 f8f8 ... f8f8 f8f8   \n\</u></td></tr>
<tr><th id="248">248</th><td><u>pand      %%xmm5, %%xmm2        # g7g6g5g4 g3g2____ g7g6g5g4 g3g2____       \n\</u></td></tr>
<tr><th id="249">249</th><td><u>psrlw     $3,%%xmm0             # ______b7 b6b5b4b3 ______b7 b6b5b4b3       \n\</u></td></tr>
<tr><th id="250">250</th><td><u>pxor      %%xmm4, %%xmm4        # zero mm4                                  \n\</u></td></tr>
<tr><th id="251">251</th><td><u>movdqa    %%xmm0, %%xmm5        # Copy B15-B0                               \n\</u></td></tr>
<tr><th id="252">252</th><td><u>movdqa    %%xmm2, %%xmm7        # Copy G15-G0                               \n\</u></td></tr>
<tr><th id="253">253</th><td><u>                                                                            \n\</u></td></tr>
<tr><th id="254">254</th><td><u># convert rgb24 plane to rgb16 pack for pixel 0-7                           \n\</u></td></tr>
<tr><th id="255">255</th><td><u>punpcklbw %%xmm4, %%xmm2        # ________ ________ g7g6g5g4 g3g2____       \n\</u></td></tr>
<tr><th id="256">256</th><td><u>punpcklbw %%xmm1, %%xmm0        # r7r6r5r4 r3______ ______b7 b6b5b4b3       \n\</u></td></tr>
<tr><th id="257">257</th><td><u>psllw     $3,%%xmm2             # ________ __g7g6g5 g4g3g2__ ________       \n\</u></td></tr>
<tr><th id="258">258</th><td><u>por       %%xmm2, %%xmm0        # r7r6r5r4 r3g7g6g5 g4g3g2b7 b6b5b4b3       \n\</u></td></tr>
<tr><th id="259">259</th><td><u>movdqu    %%xmm0, (%3)          # store pixel 0-7                           \n\</u></td></tr>
<tr><th id="260">260</th><td><u>                                                                            \n\</u></td></tr>
<tr><th id="261">261</th><td><u># convert rgb24 plane to rgb16 pack for pixel 8-15                          \n\</u></td></tr>
<tr><th id="262">262</th><td><u>punpckhbw %%xmm4, %%xmm7        # ________ ________ g7g6g5g4 g3g2____       \n\</u></td></tr>
<tr><th id="263">263</th><td><u>punpckhbw %%xmm1, %%xmm5        # r7r6r5r4 r3______ ______b7 b6b5b4b3       \n\</u></td></tr>
<tr><th id="264">264</th><td><u>psllw     $3,%%xmm7             # ________ __g7g6g5 g4g3g2__ ________       \n\</u></td></tr>
<tr><th id="265">265</th><td><u>por       %%xmm7, %%xmm5        # r7r6r5r4 r3g7g6g5 g4g3g2b7 b6b5b4b3       \n\</u></td></tr>
<tr><th id="266">266</th><td><u>movdqu    %%xmm5, 16(%3)        # store pixel 4-7                           \n\</u></td></tr>
<tr><th id="267">267</th><td><u>"</u></td></tr>
<tr><th id="268">268</th><td></td></tr>
<tr><th id="269">269</th><td><u>#define <dfn class="macro" id="_M/SSE2_UNPACK_32_ARGB_ALIGNED" data-ref="_M/SSE2_UNPACK_32_ARGB_ALIGNED">SSE2_UNPACK_32_ARGB_ALIGNED</dfn> "                                       \n\</u></td></tr>
<tr><th id="270">270</th><td><u>pxor      %%xmm3, %%xmm3  # zero xmm3                                       \n\</u></td></tr>
<tr><th id="271">271</th><td><u>movdqa    %%xmm0, %%xmm4  #               B7 B6 B5 B4 B3 B2 B1 B0           \n\</u></td></tr>
<tr><th id="272">272</th><td><u>punpcklbw %%xmm2, %%xmm4  #               G3 B3 G2 B2 G1 B1 G0 B0           \n\</u></td></tr>
<tr><th id="273">273</th><td><u>movdqa    %%xmm1, %%xmm5  #               R7 R6 R5 R4 R3 R2 R1 R0           \n\</u></td></tr>
<tr><th id="274">274</th><td><u>punpcklbw %%xmm3, %%xmm5  #               00 R3 00 R2 00 R1 00 R0           \n\</u></td></tr>
<tr><th id="275">275</th><td><u>movdqa    %%xmm4, %%xmm6  #               G3 B3 G2 B2 G1 B1 G0 B0           \n\</u></td></tr>
<tr><th id="276">276</th><td><u>punpcklwd %%xmm5, %%xmm4  #               00 R1 B1 G1 00 R0 B0 G0           \n\</u></td></tr>
<tr><th id="277">277</th><td><u>movntdq   %%xmm4, (%3)    # Store ARGB3 ARGB2 ARGB1 ARGB0                   \n\</u></td></tr>
<tr><th id="278">278</th><td><u>punpckhwd %%xmm5, %%xmm6  #               00 R3 B3 G3 00 R2 B2 G2           \n\</u></td></tr>
<tr><th id="279">279</th><td><u>movntdq   %%xmm6, 16(%3)  # Store ARGB7 ARGB6 ARGB5 ARGB4                   \n\</u></td></tr>
<tr><th id="280">280</th><td><u>punpckhbw %%xmm2, %%xmm0  #               G7 B7 G6 B6 G5 B5 G4 B4           \n\</u></td></tr>
<tr><th id="281">281</th><td><u>punpckhbw %%xmm3, %%xmm1  #               00 R7 00 R6 00 R5 00 R4           \n\</u></td></tr>
<tr><th id="282">282</th><td><u>movdqa    %%xmm0, %%xmm5  #               G7 B7 G6 B6 G5 B5 G4 B4           \n\</u></td></tr>
<tr><th id="283">283</th><td><u>punpcklwd %%xmm1, %%xmm5  #               00 R5 B5 G5 00 R4 B4 G4           \n\</u></td></tr>
<tr><th id="284">284</th><td><u>movntdq   %%xmm5, 32(%3)  # Store ARGB11 ARGB10 ARGB9 ARGB8                 \n\</u></td></tr>
<tr><th id="285">285</th><td><u>punpckhwd %%xmm1, %%xmm0  #               00 R7 B7 G7 00 R6 B6 G6           \n\</u></td></tr>
<tr><th id="286">286</th><td><u>movntdq   %%xmm0, 48(%3)  # Store ARGB15 ARGB14 ARGB13 ARGB12               \n\</u></td></tr>
<tr><th id="287">287</th><td><u>"</u></td></tr>
<tr><th id="288">288</th><td></td></tr>
<tr><th id="289">289</th><td><u>#define <dfn class="macro" id="_M/SSE2_UNPACK_32_ARGB_UNALIGNED" data-ref="_M/SSE2_UNPACK_32_ARGB_UNALIGNED">SSE2_UNPACK_32_ARGB_UNALIGNED</dfn> "                                     \n\</u></td></tr>
<tr><th id="290">290</th><td><u>pxor      %%xmm3, %%xmm3  # zero xmm3                                       \n\</u></td></tr>
<tr><th id="291">291</th><td><u>movdqa    %%xmm0, %%xmm4  #               B7 B6 B5 B4 B3 B2 B1 B0           \n\</u></td></tr>
<tr><th id="292">292</th><td><u>punpcklbw %%xmm2, %%xmm4  #               G3 B3 G2 B2 G1 B1 G0 B0           \n\</u></td></tr>
<tr><th id="293">293</th><td><u>movdqa    %%xmm1, %%xmm5  #               R7 R6 R5 R4 R3 R2 R1 R0           \n\</u></td></tr>
<tr><th id="294">294</th><td><u>punpcklbw %%xmm3, %%xmm5  #               00 R3 00 R2 00 R1 00 R0           \n\</u></td></tr>
<tr><th id="295">295</th><td><u>movdqa    %%xmm4, %%xmm6  #               G3 B3 G2 B2 G1 B1 G0 B0           \n\</u></td></tr>
<tr><th id="296">296</th><td><u>punpcklwd %%xmm5, %%xmm4  #               00 R1 B1 G1 00 R0 B0 G0           \n\</u></td></tr>
<tr><th id="297">297</th><td><u>movdqu    %%xmm4, (%3)    # Store ARGB3 ARGB2 ARGB1 ARGB0                   \n\</u></td></tr>
<tr><th id="298">298</th><td><u>punpckhwd %%xmm5, %%xmm6  #               00 R3 B3 G3 00 R2 B2 G2           \n\</u></td></tr>
<tr><th id="299">299</th><td><u>movdqu    %%xmm6, 16(%3)  # Store ARGB7 ARGB6 ARGB5 ARGB4                   \n\</u></td></tr>
<tr><th id="300">300</th><td><u>punpckhbw %%xmm2, %%xmm0  #               G7 B7 G6 B6 G5 B5 G4 B4           \n\</u></td></tr>
<tr><th id="301">301</th><td><u>punpckhbw %%xmm3, %%xmm1  #               00 R7 00 R6 00 R5 00 R4           \n\</u></td></tr>
<tr><th id="302">302</th><td><u>movdqa    %%xmm0, %%xmm5  #               G7 B7 G6 B6 G5 B5 G4 B4           \n\</u></td></tr>
<tr><th id="303">303</th><td><u>punpcklwd %%xmm1, %%xmm5  #               00 R5 B5 G5 00 R4 B4 G4           \n\</u></td></tr>
<tr><th id="304">304</th><td><u>movdqu    %%xmm5, 32(%3)  # Store ARGB11 ARGB10 ARGB9 ARGB8                 \n\</u></td></tr>
<tr><th id="305">305</th><td><u>punpckhwd %%xmm1, %%xmm0  #               00 R7 B7 G7 00 R6 B6 G6           \n\</u></td></tr>
<tr><th id="306">306</th><td><u>movdqu    %%xmm0, 48(%3)  # Store ARGB15 ARGB14 ARGB13 ARGB12               \n\</u></td></tr>
<tr><th id="307">307</th><td><u>"</u></td></tr>
<tr><th id="308">308</th><td></td></tr>
<tr><th id="309">309</th><td><u>#define <dfn class="macro" id="_M/SSE2_UNPACK_32_RGBA_ALIGNED" data-ref="_M/SSE2_UNPACK_32_RGBA_ALIGNED">SSE2_UNPACK_32_RGBA_ALIGNED</dfn> "                                       \n\</u></td></tr>
<tr><th id="310">310</th><td><u>pxor      %%xmm3, %%xmm3  # zero mm3                                        \n\</u></td></tr>
<tr><th id="311">311</th><td><u>movdqa    %%xmm2, %%xmm4  #                 G7 G6 G5 G4 G3 G2 G1 G0         \n\</u></td></tr>
<tr><th id="312">312</th><td><u>punpcklbw %%xmm1, %%xmm4  #                 R3 G3 R2 G2 R1 G1 R0 G0         \n\</u></td></tr>
<tr><th id="313">313</th><td><u>punpcklbw %%xmm0, %%xmm3  #                 B3 00 B2 00 B1 00 B0 00         \n\</u></td></tr>
<tr><th id="314">314</th><td><u>movdqa    %%xmm3, %%xmm5  #                 R3 00 R2 00 R1 00 R0 00         \n\</u></td></tr>
<tr><th id="315">315</th><td><u>punpcklwd %%xmm4, %%xmm3  #                 R1 G1 B1 00 R0 B0 G0 00         \n\</u></td></tr>
<tr><th id="316">316</th><td><u>movntdq   %%xmm3, (%3)    # Store RGBA3 RGBA2 RGBA1 RGBA0                   \n\</u></td></tr>
<tr><th id="317">317</th><td><u>punpckhwd %%xmm4, %%xmm5  #                 R3 G3 B3 00 R2 G2 B2 00         \n\</u></td></tr>
<tr><th id="318">318</th><td><u>movntdq   %%xmm5, 16(%3)  # Store RGBA7 RGBA6 RGBA5 RGBA4                   \n\</u></td></tr>
<tr><th id="319">319</th><td><u>pxor      %%xmm6, %%xmm6  # zero mm6                                        \n\</u></td></tr>
<tr><th id="320">320</th><td><u>punpckhbw %%xmm1, %%xmm2  #                 R7 G7 R6 G6 R5 G5 R4 G4         \n\</u></td></tr>
<tr><th id="321">321</th><td><u>punpckhbw %%xmm0, %%xmm6  #                 B7 00 B6 00 B5 00 B4 00         \n\</u></td></tr>
<tr><th id="322">322</th><td><u>movdqa    %%xmm6, %%xmm0  #                 B7 00 B6 00 B5 00 B4 00         \n\</u></td></tr>
<tr><th id="323">323</th><td><u>punpcklwd %%xmm2, %%xmm6  #                 R5 G5 B5 00 R4 G4 B4 00         \n\</u></td></tr>
<tr><th id="324">324</th><td><u>movntdq   %%xmm6, 32(%3)  # Store BGRA11 BGRA10 BGRA9 RGBA8                 \n\</u></td></tr>
<tr><th id="325">325</th><td><u>punpckhwd %%xmm2, %%xmm0  #                 R7 G7 B7 00 R6 G6 B6 00         \n\</u></td></tr>
<tr><th id="326">326</th><td><u>movntdq   %%xmm0, 48(%3)  # Store RGBA15 RGBA14 RGBA13 RGBA12               \n\</u></td></tr>
<tr><th id="327">327</th><td><u>"</u></td></tr>
<tr><th id="328">328</th><td></td></tr>
<tr><th id="329">329</th><td><u>#define <dfn class="macro" id="_M/SSE2_UNPACK_32_RGBA_UNALIGNED" data-ref="_M/SSE2_UNPACK_32_RGBA_UNALIGNED">SSE2_UNPACK_32_RGBA_UNALIGNED</dfn> "                                     \n\</u></td></tr>
<tr><th id="330">330</th><td><u>pxor      %%xmm3, %%xmm3  # zero mm3                                        \n\</u></td></tr>
<tr><th id="331">331</th><td><u>movdqa    %%xmm2, %%xmm4  #                 G7 G6 G5 G4 G3 G2 G1 G0         \n\</u></td></tr>
<tr><th id="332">332</th><td><u>punpcklbw %%xmm1, %%xmm4  #                 R3 G3 R2 G2 R1 G1 R0 G0         \n\</u></td></tr>
<tr><th id="333">333</th><td><u>punpcklbw %%xmm0, %%xmm3  #                 B3 00 B2 00 B1 00 B0 00         \n\</u></td></tr>
<tr><th id="334">334</th><td><u>movdqa    %%xmm3, %%xmm5  #                 R3 00 R2 00 R1 00 R0 00         \n\</u></td></tr>
<tr><th id="335">335</th><td><u>punpcklwd %%xmm4, %%xmm3  #                 R1 G1 B1 00 R0 B0 G0 00         \n\</u></td></tr>
<tr><th id="336">336</th><td><u>movdqu    %%xmm3, (%3)    # Store RGBA3 RGBA2 RGBA1 RGBA0                   \n\</u></td></tr>
<tr><th id="337">337</th><td><u>punpckhwd %%xmm4, %%xmm5  #                 R3 G3 B3 00 R2 G2 B2 00         \n\</u></td></tr>
<tr><th id="338">338</th><td><u>movdqu    %%xmm5, 16(%3)  # Store RGBA7 RGBA6 RGBA5 RGBA4                   \n\</u></td></tr>
<tr><th id="339">339</th><td><u>pxor      %%xmm6, %%xmm6  # zero mm6                                        \n\</u></td></tr>
<tr><th id="340">340</th><td><u>punpckhbw %%xmm1, %%xmm2  #                 R7 G7 R6 G6 R5 G5 R4 G4         \n\</u></td></tr>
<tr><th id="341">341</th><td><u>punpckhbw %%xmm0, %%xmm6  #                 B7 00 B6 00 B5 00 B4 00         \n\</u></td></tr>
<tr><th id="342">342</th><td><u>movdqa    %%xmm6, %%xmm0  #                 B7 00 B6 00 B5 00 B4 00         \n\</u></td></tr>
<tr><th id="343">343</th><td><u>punpcklwd %%xmm2, %%xmm6  #                 R5 G5 B5 00 R4 G4 B4 00         \n\</u></td></tr>
<tr><th id="344">344</th><td><u>movdqu    %%xmm6, 32(%3)  # Store RGBA11 RGBA10 RGBA9 RGBA8                 \n\</u></td></tr>
<tr><th id="345">345</th><td><u>punpckhwd %%xmm2, %%xmm0  #                 R7 G7 B7 00 R6 G6 B6 00         \n\</u></td></tr>
<tr><th id="346">346</th><td><u>movdqu    %%xmm0, 48(%3)  # Store RGBA15 RGBA14 RGBA13 RGBA12               \n\</u></td></tr>
<tr><th id="347">347</th><td><u>"</u></td></tr>
<tr><th id="348">348</th><td></td></tr>
<tr><th id="349">349</th><td><u>#define <dfn class="macro" id="_M/SSE2_UNPACK_32_BGRA_ALIGNED" data-ref="_M/SSE2_UNPACK_32_BGRA_ALIGNED">SSE2_UNPACK_32_BGRA_ALIGNED</dfn> "                                       \n\</u></td></tr>
<tr><th id="350">350</th><td><u>pxor      %%xmm3, %%xmm3  # zero mm3                                        \n\</u></td></tr>
<tr><th id="351">351</th><td><u>movdqa    %%xmm2, %%xmm4  #                 G7 G6 G5 G4 G3 G2 G1 G0         \n\</u></td></tr>
<tr><th id="352">352</th><td><u>punpcklbw %%xmm0, %%xmm4  #                 B3 G3 B2 G2 B1 G1 B0 G0         \n\</u></td></tr>
<tr><th id="353">353</th><td><u>punpcklbw %%xmm1, %%xmm3  #                 R3 00 R2 00 R1 00 R0 00         \n\</u></td></tr>
<tr><th id="354">354</th><td><u>movdqa    %%xmm3, %%xmm5  #                 R3 00 R2 00 R1 00 R0 00         \n\</u></td></tr>
<tr><th id="355">355</th><td><u>punpcklwd %%xmm4, %%xmm3  #                 B1 G1 R1 00 B0 G0 R0 00         \n\</u></td></tr>
<tr><th id="356">356</th><td><u>movntdq   %%xmm3, (%3)    # Store BGRA3 BGRA2 BGRA1 BGRA0                   \n\</u></td></tr>
<tr><th id="357">357</th><td><u>punpckhwd %%xmm4, %%xmm5  #                 B3 G3 R3 00 B2 G2 R2 00         \n\</u></td></tr>
<tr><th id="358">358</th><td><u>movntdq   %%xmm5, 16(%3)  # Store BGRA7 BGRA6 BGRA5 BGRA4                   \n\</u></td></tr>
<tr><th id="359">359</th><td><u>pxor      %%xmm6, %%xmm6  # zero mm6                                        \n\</u></td></tr>
<tr><th id="360">360</th><td><u>punpckhbw %%xmm0, %%xmm2  #                 B7 G7 B6 G6 B5 G5 B4 G4         \n\</u></td></tr>
<tr><th id="361">361</th><td><u>punpckhbw %%xmm1, %%xmm6  #                 R7 00 R6 00 R5 00 R4 00         \n\</u></td></tr>
<tr><th id="362">362</th><td><u>movdqa    %%xmm6, %%xmm0  #                 R7 00 R6 00 R5 00 R4 00         \n\</u></td></tr>
<tr><th id="363">363</th><td><u>punpcklwd %%xmm2, %%xmm6  #                 B5 G5 R5 00 B4 G4 R4 00         \n\</u></td></tr>
<tr><th id="364">364</th><td><u>movntdq   %%xmm6, 32(%3)  # Store BGRA11 BGRA10 BGRA9 BGRA8                 \n\</u></td></tr>
<tr><th id="365">365</th><td><u>punpckhwd %%xmm2, %%xmm0  #                 B7 G7 R7 00 B6 G6 R6 00         \n\</u></td></tr>
<tr><th id="366">366</th><td><u>movntdq   %%xmm0, 48(%3)  # Store BGRA15 BGRA14 BGRA13 BGRA12               \n\</u></td></tr>
<tr><th id="367">367</th><td><u>"</u></td></tr>
<tr><th id="368">368</th><td></td></tr>
<tr><th id="369">369</th><td><u>#define <dfn class="macro" id="_M/SSE2_UNPACK_32_BGRA_UNALIGNED" data-ref="_M/SSE2_UNPACK_32_BGRA_UNALIGNED">SSE2_UNPACK_32_BGRA_UNALIGNED</dfn> "                                     \n\</u></td></tr>
<tr><th id="370">370</th><td><u>pxor      %%xmm3, %%xmm3  # zero mm3                                        \n\</u></td></tr>
<tr><th id="371">371</th><td><u>movdqa    %%xmm2, %%xmm4  #                 G7 G6 G5 G4 G3 G2 G1 G0         \n\</u></td></tr>
<tr><th id="372">372</th><td><u>punpcklbw %%xmm0, %%xmm4  #                 B3 G3 B2 G2 B1 G1 B0 G0         \n\</u></td></tr>
<tr><th id="373">373</th><td><u>punpcklbw %%xmm1, %%xmm3  #                 R3 00 R2 00 R1 00 R0 00         \n\</u></td></tr>
<tr><th id="374">374</th><td><u>movdqa    %%xmm3, %%xmm5  #                 R3 00 R2 00 R1 00 R0 00         \n\</u></td></tr>
<tr><th id="375">375</th><td><u>punpcklwd %%xmm4, %%xmm3  #                 B1 G1 R1 00 B0 G0 R0 00         \n\</u></td></tr>
<tr><th id="376">376</th><td><u>movdqu    %%xmm3, (%3)    # Store BGRA3 BGRA2 BGRA1 BGRA0                   \n\</u></td></tr>
<tr><th id="377">377</th><td><u>punpckhwd %%xmm4, %%xmm5  #                 B3 G3 R3 00 B2 G2 R2 00         \n\</u></td></tr>
<tr><th id="378">378</th><td><u>movdqu    %%xmm5, 16(%3)  # Store BGRA7 BGRA6 BGRA5 BGRA4                   \n\</u></td></tr>
<tr><th id="379">379</th><td><u>pxor      %%xmm6, %%xmm6  # zero mm6                                        \n\</u></td></tr>
<tr><th id="380">380</th><td><u>punpckhbw %%xmm0, %%xmm2  #                 B7 G7 B6 G6 B5 G5 B4 G4         \n\</u></td></tr>
<tr><th id="381">381</th><td><u>punpckhbw %%xmm1, %%xmm6  #                 R7 00 R6 00 R5 00 R4 00         \n\</u></td></tr>
<tr><th id="382">382</th><td><u>movdqa    %%xmm6, %%xmm0  #                 R7 00 R6 00 R5 00 R4 00         \n\</u></td></tr>
<tr><th id="383">383</th><td><u>punpcklwd %%xmm2, %%xmm6  #                 B5 G5 R5 00 B4 G4 R4 00         \n\</u></td></tr>
<tr><th id="384">384</th><td><u>movdqu    %%xmm6, 32(%3)  # Store BGRA11 BGRA10 BGRA9 BGRA8                 \n\</u></td></tr>
<tr><th id="385">385</th><td><u>punpckhwd %%xmm2, %%xmm0  #                 B7 G7 R7 00 B6 G6 R6 00         \n\</u></td></tr>
<tr><th id="386">386</th><td><u>movdqu    %%xmm0, 48(%3)  # Store BGRA15 BGRA14 BGRA13 BGRA12               \n\</u></td></tr>
<tr><th id="387">387</th><td><u>"</u></td></tr>
<tr><th id="388">388</th><td></td></tr>
<tr><th id="389">389</th><td><u>#define <dfn class="macro" id="_M/SSE2_UNPACK_32_ABGR_ALIGNED" data-ref="_M/SSE2_UNPACK_32_ABGR_ALIGNED">SSE2_UNPACK_32_ABGR_ALIGNED</dfn> "                                       \n\</u></td></tr>
<tr><th id="390">390</th><td><u>pxor      %%xmm3, %%xmm3  # zero mm3                                        \n\</u></td></tr>
<tr><th id="391">391</th><td><u>movdqa    %%xmm1, %%xmm4  #                 R7 R6 R5 R4 R3 R2 R1 R0         \n\</u></td></tr>
<tr><th id="392">392</th><td><u>punpcklbw %%xmm2, %%xmm4  #                 G3 R3 G2 R2 G1 R1 G0 R0         \n\</u></td></tr>
<tr><th id="393">393</th><td><u>movdqa    %%xmm0, %%xmm5  #                 B7 B6 B5 B4 B3 B2 B1 B0         \n\</u></td></tr>
<tr><th id="394">394</th><td><u>punpcklbw %%xmm3, %%xmm5  #                 00 B3 00 B2 00 B1 00 B0         \n\</u></td></tr>
<tr><th id="395">395</th><td><u>movdqa    %%xmm4, %%xmm6  #                 G3 R3 G2 R2 G1 R1 G0 R0         \n\</u></td></tr>
<tr><th id="396">396</th><td><u>punpcklwd %%xmm5, %%xmm4  #                 00 B1 G1 R1 00 B0 G0 R0         \n\</u></td></tr>
<tr><th id="397">397</th><td><u>movntdq   %%xmm4, (%3)    # Store ABGR3 ABGR2 ABGR1 ABGR0                   \n\</u></td></tr>
<tr><th id="398">398</th><td><u>punpckhwd %%xmm5, %%xmm6  #                 00 B3 G3 R3 00 B2 G2 R2         \n\</u></td></tr>
<tr><th id="399">399</th><td><u>movntdq   %%xmm6, 16(%3)  # Store ABGR7 ABGR6 ABGR5 ABGR4                   \n\</u></td></tr>
<tr><th id="400">400</th><td><u>punpckhbw %%xmm2, %%xmm1  #                 G7 R7 G6 R6 G5 R5 G4 R4         \n\</u></td></tr>
<tr><th id="401">401</th><td><u>punpckhbw %%xmm3, %%xmm0  #                 00 B7 00 B6 00 B5 00 B4         \n\</u></td></tr>
<tr><th id="402">402</th><td><u>movdqa    %%xmm1, %%xmm2  #                 G7 R7 G6 R6 G5 R5 G4 R4         \n\</u></td></tr>
<tr><th id="403">403</th><td><u>punpcklwd %%xmm0, %%xmm1  #                 00 B5 G5 R5 00 B4 G4 R4         \n\</u></td></tr>
<tr><th id="404">404</th><td><u>movntdq   %%xmm1, 32(%3)  # Store ABGR11 ABGR10 ABGR9 ABGR8                 \n\</u></td></tr>
<tr><th id="405">405</th><td><u>punpckhwd %%xmm0, %%xmm2  #                 B7 G7 R7 00 B6 G6 R6 00         \n\</u></td></tr>
<tr><th id="406">406</th><td><u>movntdq   %%xmm2, 48(%3)  # Store ABGR15 ABGR14 ABGR13 ABGR12               \n\</u></td></tr>
<tr><th id="407">407</th><td><u>"</u></td></tr>
<tr><th id="408">408</th><td></td></tr>
<tr><th id="409">409</th><td><u>#define <dfn class="macro" id="_M/SSE2_UNPACK_32_ABGR_UNALIGNED" data-ref="_M/SSE2_UNPACK_32_ABGR_UNALIGNED">SSE2_UNPACK_32_ABGR_UNALIGNED</dfn> "                                     \n\</u></td></tr>
<tr><th id="410">410</th><td><u>pxor      %%xmm3, %%xmm3  # zero mm3                                        \n\</u></td></tr>
<tr><th id="411">411</th><td><u>movdqa    %%xmm1, %%xmm4  #                 R7 R6 R5 R4 R3 R2 R1 R0         \n\</u></td></tr>
<tr><th id="412">412</th><td><u>punpcklbw %%xmm2, %%xmm4  #                 G3 R3 G2 R2 G1 R1 G0 R0         \n\</u></td></tr>
<tr><th id="413">413</th><td><u>movdqa    %%xmm0, %%xmm5  #                 B7 B6 B5 B4 B3 B2 B1 B0         \n\</u></td></tr>
<tr><th id="414">414</th><td><u>punpcklbw %%xmm3, %%xmm5  #                 00 B3 00 B2 00 B1 00 B0         \n\</u></td></tr>
<tr><th id="415">415</th><td><u>movdqa    %%xmm4, %%xmm6  #                 G3 R3 G2 R2 G1 R1 G0 R0         \n\</u></td></tr>
<tr><th id="416">416</th><td><u>punpcklwd %%xmm5, %%xmm4  #                 00 B1 G1 R1 00 B0 G0 R0         \n\</u></td></tr>
<tr><th id="417">417</th><td><u>movdqu    %%xmm4, (%3)    # Store ABGR3 ABGR2 ABGR1 ABGR0                   \n\</u></td></tr>
<tr><th id="418">418</th><td><u>punpckhwd %%xmm5, %%xmm6  #                 00 B3 G3 R3 00 B2 G2 R2         \n\</u></td></tr>
<tr><th id="419">419</th><td><u>movdqu    %%xmm6, 16(%3)  # Store ABGR7 ABGR6 ABGR5 ABGR4                   \n\</u></td></tr>
<tr><th id="420">420</th><td><u>punpckhbw %%xmm2, %%xmm1  #                 G7 R7 G6 R6 G5 R5 G4 R4         \n\</u></td></tr>
<tr><th id="421">421</th><td><u>punpckhbw %%xmm3, %%xmm0  #                 00 B7 00 B6 00 B5 00 B4         \n\</u></td></tr>
<tr><th id="422">422</th><td><u>movdqa    %%xmm1, %%xmm2  #                 R7 00 R6 00 R5 00 R4 00         \n\</u></td></tr>
<tr><th id="423">423</th><td><u>punpcklwd %%xmm0, %%xmm1  #                 00 B5 G5 R5 00 B4 G4 R4         \n\</u></td></tr>
<tr><th id="424">424</th><td><u>movdqu    %%xmm1, 32(%3)  # Store ABGR11 ABGR10 ABGR9 ABGR8                 \n\</u></td></tr>
<tr><th id="425">425</th><td><u>punpckhwd %%xmm0, %%xmm2  #                 B7 G7 R7 00 B6 G6 R6 00         \n\</u></td></tr>
<tr><th id="426">426</th><td><u>movdqu    %%xmm2, 48(%3)  # Store ABGR15 ABGR14 ABGR13 ABGR12               \n\</u></td></tr>
<tr><th id="427">427</th><td><u>"</u></td></tr>
<tr><th id="428">428</th><td></td></tr>
<tr><th id="429">429</th><td><u>#<span data-ppcond="22">elif</span> defined(HAVE_SSE2_INTRINSICS)</u></td></tr>
<tr><th id="430">430</th><td></td></tr>
<tr><th id="431">431</th><td><i>/* SSE2 intrinsics */</i></td></tr>
<tr><th id="432">432</th><td></td></tr>
<tr><th id="433">433</th><td><u>#include &lt;emmintrin.h&gt;</u></td></tr>
<tr><th id="434">434</th><td></td></tr>
<tr><th id="435">435</th><td><u>#define SSE2_CALL(SSE2_INSTRUCTIONS)        \</u></td></tr>
<tr><th id="436">436</th><td><u>    do {                                    \</u></td></tr>
<tr><th id="437">437</th><td><u>        __m128i xmm0, xmm1, xmm2, xmm3,     \</u></td></tr>
<tr><th id="438">438</th><td><u>                xmm4, xmm5, xmm6, xmm7;     \</u></td></tr>
<tr><th id="439">439</th><td><u>        SSE2_INSTRUCTIONS                   \</u></td></tr>
<tr><th id="440">440</th><td><u>    } while(0)</u></td></tr>
<tr><th id="441">441</th><td></td></tr>
<tr><th id="442">442</th><td><u>#define SSE2_END  _mm_sfence()</u></td></tr>
<tr><th id="443">443</th><td></td></tr>
<tr><th id="444">444</th><td><u>#define SSE2_INIT_16_ALIGNED                \</u></td></tr>
<tr><th id="445">445</th><td><u>    xmm0 = _mm_loadl_epi64((__m128i *)p_u); \</u></td></tr>
<tr><th id="446">446</th><td><u>    xmm1 = _mm_loadl_epi64((__m128i *)p_v); \</u></td></tr>
<tr><th id="447">447</th><td><u>    xmm4 = _mm_setzero_si128();             \</u></td></tr>
<tr><th id="448">448</th><td><u>    xmm6 = _mm_load_si128((__m128i *)p_y);</u></td></tr>
<tr><th id="449">449</th><td></td></tr>
<tr><th id="450">450</th><td><u>#define SSE2_INIT_16_UNALIGNED              \</u></td></tr>
<tr><th id="451">451</th><td><u>    xmm0 = _mm_loadl_epi64((__m128i *)p_u); \</u></td></tr>
<tr><th id="452">452</th><td><u>    xmm1 = _mm_loadl_epi64((__m128i *)p_v); \</u></td></tr>
<tr><th id="453">453</th><td><u>    xmm4 = _mm_setzero_si128();             \</u></td></tr>
<tr><th id="454">454</th><td><u>    xmm6 = _mm_loadu_si128((__m128i *)p_y); \</u></td></tr>
<tr><th id="455">455</th><td><u>    _mm_prefetch(p_buffer, _MM_HINT_NTA);</u></td></tr>
<tr><th id="456">456</th><td></td></tr>
<tr><th id="457">457</th><td><u>#define SSE2_INIT_32_ALIGNED                \</u></td></tr>
<tr><th id="458">458</th><td><u>    xmm0 = _mm_loadl_epi64((__m128i *)p_u); \</u></td></tr>
<tr><th id="459">459</th><td><u>    xmm1 = _mm_loadl_epi64((__m128i *)p_v); \</u></td></tr>
<tr><th id="460">460</th><td><u>    xmm4 = _mm_setzero_si128();             \</u></td></tr>
<tr><th id="461">461</th><td><u>    xmm6 = _mm_load_si128((__m128i *)p_y);</u></td></tr>
<tr><th id="462">462</th><td></td></tr>
<tr><th id="463">463</th><td><u>#define SSE2_INIT_32_UNALIGNED              \</u></td></tr>
<tr><th id="464">464</th><td><u>    xmm0 = _mm_loadl_epi64((__m128i *)p_u); \</u></td></tr>
<tr><th id="465">465</th><td><u>    xmm1 = _mm_loadl_epi64((__m128i *)p_v); \</u></td></tr>
<tr><th id="466">466</th><td><u>    xmm4 = _mm_setzero_si128();             \</u></td></tr>
<tr><th id="467">467</th><td><u>    xmm6 = _mm_loadu_si128((__m128i *)p_y); \</u></td></tr>
<tr><th id="468">468</th><td><u>    _mm_prefetch(p_buffer, _MM_HINT_NTA);</u></td></tr>
<tr><th id="469">469</th><td></td></tr>
<tr><th id="470">470</th><td><u>#define SSE2_YUV_MUL                        \</u></td></tr>
<tr><th id="471">471</th><td><u>    xmm0 = _mm_unpacklo_epi8(xmm0, xmm4);   \</u></td></tr>
<tr><th id="472">472</th><td><u>    xmm1 = _mm_unpacklo_epi8(xmm1, xmm4);   \</u></td></tr>
<tr><th id="473">473</th><td><u>    xmm5 = _mm_set1_epi32(0x00800080UL);    \</u></td></tr>
<tr><th id="474">474</th><td><u>    xmm0 = _mm_subs_epi16(xmm0, xmm5);      \</u></td></tr>
<tr><th id="475">475</th><td><u>    xmm1 = _mm_subs_epi16(xmm1, xmm5);      \</u></td></tr>
<tr><th id="476">476</th><td><u>    xmm0 = _mm_slli_epi16(xmm0, 3);         \</u></td></tr>
<tr><th id="477">477</th><td><u>    xmm1 = _mm_slli_epi16(xmm1, 3);         \</u></td></tr>
<tr><th id="478">478</th><td><u>    xmm2 = xmm0;                            \</u></td></tr>
<tr><th id="479">479</th><td><u>    xmm3 = xmm1;                            \</u></td></tr>
<tr><th id="480">480</th><td><u>    xmm5 = _mm_set1_epi32(0xf37df37dUL);    \</u></td></tr>
<tr><th id="481">481</th><td><u>    xmm2 = _mm_mulhi_epi16(xmm2, xmm5);     \</u></td></tr>
<tr><th id="482">482</th><td><u>    xmm5 = _mm_set1_epi32(0xe5fce5fcUL);    \</u></td></tr>
<tr><th id="483">483</th><td><u>    xmm3 = _mm_mulhi_epi16(xmm3, xmm5);     \</u></td></tr>
<tr><th id="484">484</th><td><u>    xmm5 = _mm_set1_epi32(0x40934093UL);    \</u></td></tr>
<tr><th id="485">485</th><td><u>    xmm0 = _mm_mulhi_epi16(xmm0, xmm5);     \</u></td></tr>
<tr><th id="486">486</th><td><u>    xmm5 = _mm_set1_epi32(0x33123312UL);    \</u></td></tr>
<tr><th id="487">487</th><td><u>    xmm1 = _mm_mulhi_epi16(xmm1, xmm5);     \</u></td></tr>
<tr><th id="488">488</th><td><u>    xmm2 = _mm_adds_epi16(xmm2, xmm3);      \</u></td></tr>
<tr><th id="489">489</th><td><u>    \</u></td></tr>
<tr><th id="490">490</th><td><u>    xmm5 = _mm_set1_epi32(0x10101010UL);    \</u></td></tr>
<tr><th id="491">491</th><td><u>    xmm6 = _mm_subs_epu8(xmm6, xmm5);       \</u></td></tr>
<tr><th id="492">492</th><td><u>    xmm7 = xmm6;                            \</u></td></tr>
<tr><th id="493">493</th><td><u>    xmm5 = _mm_set1_epi32(0x00ff00ffUL);    \</u></td></tr>
<tr><th id="494">494</th><td><u>    xmm6 = _mm_and_si128(xmm6, xmm5);       \</u></td></tr>
<tr><th id="495">495</th><td><u>    xmm7 = _mm_srli_epi16(xmm7, 8);         \</u></td></tr>
<tr><th id="496">496</th><td><u>    xmm6 = _mm_slli_epi16(xmm6, 3);         \</u></td></tr>
<tr><th id="497">497</th><td><u>    xmm7 = _mm_slli_epi16(xmm7, 3);         \</u></td></tr>
<tr><th id="498">498</th><td><u>    xmm5 = _mm_set1_epi32(0x253f253fUL);    \</u></td></tr>
<tr><th id="499">499</th><td><u>    xmm6 = _mm_mulhi_epi16(xmm6, xmm5);     \</u></td></tr>
<tr><th id="500">500</th><td><u>    xmm7 = _mm_mulhi_epi16(xmm7, xmm5);</u></td></tr>
<tr><th id="501">501</th><td></td></tr>
<tr><th id="502">502</th><td><u>#define SSE2_YUV_ADD                        \</u></td></tr>
<tr><th id="503">503</th><td><u>    xmm3 = xmm0;                            \</u></td></tr>
<tr><th id="504">504</th><td><u>    xmm4 = xmm1;                            \</u></td></tr>
<tr><th id="505">505</th><td><u>    xmm5 = xmm2;                            \</u></td></tr>
<tr><th id="506">506</th><td><u>    xmm0 = _mm_adds_epi16(xmm0, xmm6);      \</u></td></tr>
<tr><th id="507">507</th><td><u>    xmm3 = _mm_adds_epi16(xmm3, xmm7);      \</u></td></tr>
<tr><th id="508">508</th><td><u>    xmm1 = _mm_adds_epi16(xmm1, xmm6);      \</u></td></tr>
<tr><th id="509">509</th><td><u>    xmm4 = _mm_adds_epi16(xmm4, xmm7);      \</u></td></tr>
<tr><th id="510">510</th><td><u>    xmm2 = _mm_adds_epi16(xmm2, xmm6);      \</u></td></tr>
<tr><th id="511">511</th><td><u>    xmm5 = _mm_adds_epi16(xmm5, xmm7);      \</u></td></tr>
<tr><th id="512">512</th><td><u>    \</u></td></tr>
<tr><th id="513">513</th><td><u>    xmm0 = _mm_packus_epi16(xmm0, xmm0);    \</u></td></tr>
<tr><th id="514">514</th><td><u>    xmm1 = _mm_packus_epi16(xmm1, xmm1);    \</u></td></tr>
<tr><th id="515">515</th><td><u>    xmm2 = _mm_packus_epi16(xmm2, xmm2);    \</u></td></tr>
<tr><th id="516">516</th><td><u>    \</u></td></tr>
<tr><th id="517">517</th><td><u>    xmm3 = _mm_packus_epi16(xmm3, xmm3);    \</u></td></tr>
<tr><th id="518">518</th><td><u>    xmm4 = _mm_packus_epi16(xmm4, xmm4);    \</u></td></tr>
<tr><th id="519">519</th><td><u>    xmm5 = _mm_packus_epi16(xmm5, xmm5);    \</u></td></tr>
<tr><th id="520">520</th><td><u>    \</u></td></tr>
<tr><th id="521">521</th><td><u>    xmm0 = _mm_unpacklo_epi8(xmm0, xmm3);   \</u></td></tr>
<tr><th id="522">522</th><td><u>    xmm1 = _mm_unpacklo_epi8(xmm1, xmm4);   \</u></td></tr>
<tr><th id="523">523</th><td><u>    xmm2 = _mm_unpacklo_epi8(xmm2, xmm5);</u></td></tr>
<tr><th id="524">524</th><td></td></tr>
<tr><th id="525">525</th><td><u>#define SSE2_UNPACK_15_ALIGNED                      \</u></td></tr>
<tr><th id="526">526</th><td><u>    xmm5 = _mm_set1_epi32(0xf8f8f8f8UL);            \</u></td></tr>
<tr><th id="527">527</th><td><u>    xmm0 = _mm_and_si128(xmm0, xmm5);               \</u></td></tr>
<tr><th id="528">528</th><td><u>    xmm0 = _mm_srli_epi16(xmm0, 3);                 \</u></td></tr>
<tr><th id="529">529</th><td><u>    xmm2 = _mm_and_si128(xmm2, xmm5);               \</u></td></tr>
<tr><th id="530">530</th><td><u>    xmm1 = _mm_and_si128(xmm1, xmm5);               \</u></td></tr>
<tr><th id="531">531</th><td><u>    xmm1 = _mm_srli_epi16(xmm1, 1);                 \</u></td></tr>
<tr><th id="532">532</th><td><u>    xmm4 = _mm_setzero_si128();                     \</u></td></tr>
<tr><th id="533">533</th><td><u>    xmm5 = xmm0;                                    \</u></td></tr>
<tr><th id="534">534</th><td><u>    xmm7 = xmm2;                                    \</u></td></tr>
<tr><th id="535">535</th><td><u>    \</u></td></tr>
<tr><th id="536">536</th><td><u>    xmm2 = _mm_unpacklo_epi8(xmm2, xmm4);           \</u></td></tr>
<tr><th id="537">537</th><td><u>    xmm0 = _mm_unpacklo_epi8(xmm0, xmm1);           \</u></td></tr>
<tr><th id="538">538</th><td><u>    xmm2 = _mm_slli_epi16(xmm2, 2);                 \</u></td></tr>
<tr><th id="539">539</th><td><u>    xmm0 = _mm_or_si128(xmm0, xmm2);                \</u></td></tr>
<tr><th id="540">540</th><td><u>    _mm_stream_si128((__m128i*)p_buffer, xmm0);     \</u></td></tr>
<tr><th id="541">541</th><td><u>    \</u></td></tr>
<tr><th id="542">542</th><td><u>    xmm7 = _mm_unpackhi_epi8(xmm7, xmm4);           \</u></td></tr>
<tr><th id="543">543</th><td><u>    xmm5 = _mm_unpackhi_epi8(xmm5, xmm1);           \</u></td></tr>
<tr><th id="544">544</th><td><u>    xmm7 = _mm_slli_epi16(xmm7, 2);                 \</u></td></tr>
<tr><th id="545">545</th><td><u>    xmm5 = _mm_or_si128(xmm5, xmm7);                \</u></td></tr>
<tr><th id="546">546</th><td><u>    _mm_stream_si128((__m128i*)(p_buffer+8), xmm5);</u></td></tr>
<tr><th id="547">547</th><td></td></tr>
<tr><th id="548">548</th><td><u>#define SSE2_UNPACK_15_UNALIGNED                    \</u></td></tr>
<tr><th id="549">549</th><td><u>    xmm5 = _mm_set1_epi32(0xf8f8f8f8UL);            \</u></td></tr>
<tr><th id="550">550</th><td><u>    xmm0 = _mm_and_si128(xmm0, xmm5);               \</u></td></tr>
<tr><th id="551">551</th><td><u>    xmm0 = _mm_srli_epi16(xmm0, 3);                 \</u></td></tr>
<tr><th id="552">552</th><td><u>    xmm2 = _mm_and_si128(xmm2, xmm5);               \</u></td></tr>
<tr><th id="553">553</th><td><u>    xmm1 = _mm_and_si128(xmm1, xmm5);               \</u></td></tr>
<tr><th id="554">554</th><td><u>    xmm1 = _mm_srli_epi16(xmm1, 1);                 \</u></td></tr>
<tr><th id="555">555</th><td><u>    xmm4 = _mm_setzero_si128();                     \</u></td></tr>
<tr><th id="556">556</th><td><u>    xmm5 = xmm0;                                    \</u></td></tr>
<tr><th id="557">557</th><td><u>    xmm7 = xmm2;                                    \</u></td></tr>
<tr><th id="558">558</th><td><u>    \</u></td></tr>
<tr><th id="559">559</th><td><u>    xmm2 = _mm_unpacklo_epi8(xmm2, xmm4);           \</u></td></tr>
<tr><th id="560">560</th><td><u>    xmm0 = _mm_unpacklo_epi8(xmm0, xmm1);           \</u></td></tr>
<tr><th id="561">561</th><td><u>    xmm2 = _mm_slli_epi16(xmm2, 2);                 \</u></td></tr>
<tr><th id="562">562</th><td><u>    xmm0 = _mm_or_si128(xmm0, xmm2);                \</u></td></tr>
<tr><th id="563">563</th><td><u>    _mm_storeu_si128((__m128i*)p_buffer, xmm0);     \</u></td></tr>
<tr><th id="564">564</th><td><u>    \</u></td></tr>
<tr><th id="565">565</th><td><u>    xmm7 = _mm_unpackhi_epi8(xmm7, xmm4);           \</u></td></tr>
<tr><th id="566">566</th><td><u>    xmm5 = _mm_unpackhi_epi8(xmm5, xmm1);           \</u></td></tr>
<tr><th id="567">567</th><td><u>    xmm7 = _mm_slli_epi16(xmm7, 2);                 \</u></td></tr>
<tr><th id="568">568</th><td><u>    xmm5 = _mm_or_si128(xmm5, xmm7);                \</u></td></tr>
<tr><th id="569">569</th><td><u>    _mm_storeu_si128((__m128i*)(p_buffer+16), xmm5);</u></td></tr>
<tr><th id="570">570</th><td></td></tr>
<tr><th id="571">571</th><td><u>#define SSE2_UNPACK_16_ALIGNED                      \</u></td></tr>
<tr><th id="572">572</th><td><u>    xmm5 = _mm_set1_epi32(0xf8f8f8f8UL);            \</u></td></tr>
<tr><th id="573">573</th><td><u>    xmm0 = _mm_and_si128(xmm0, xmm5);               \</u></td></tr>
<tr><th id="574">574</th><td><u>    xmm1 = _mm_and_si128(xmm1, xmm5);               \</u></td></tr>
<tr><th id="575">575</th><td><u>    xmm5 = _mm_set1_epi32(0xfcfcfcfcUL);            \</u></td></tr>
<tr><th id="576">576</th><td><u>    xmm2 = _mm_and_si128(xmm2, xmm5);               \</u></td></tr>
<tr><th id="577">577</th><td><u>    xmm0 = _mm_srli_epi16(xmm0, 3);                 \</u></td></tr>
<tr><th id="578">578</th><td><u>    xmm4 = _mm_setzero_si128();                     \</u></td></tr>
<tr><th id="579">579</th><td><u>    xmm5 = xmm0;                                    \</u></td></tr>
<tr><th id="580">580</th><td><u>    xmm7 = xmm2;                                    \</u></td></tr>
<tr><th id="581">581</th><td><u>    \</u></td></tr>
<tr><th id="582">582</th><td><u>    xmm2 = _mm_unpacklo_epi8(xmm2, xmm4);           \</u></td></tr>
<tr><th id="583">583</th><td><u>    xmm0 = _mm_unpacklo_epi8(xmm0, xmm1);           \</u></td></tr>
<tr><th id="584">584</th><td><u>    xmm2 = _mm_slli_epi16(xmm2, 3);                 \</u></td></tr>
<tr><th id="585">585</th><td><u>    xmm0 = _mm_or_si128(xmm0, xmm2);                \</u></td></tr>
<tr><th id="586">586</th><td><u>    _mm_stream_si128((__m128i*)p_buffer, xmm0);     \</u></td></tr>
<tr><th id="587">587</th><td><u>    \</u></td></tr>
<tr><th id="588">588</th><td><u>    xmm7 = _mm_unpackhi_epi8(xmm7, xmm4);           \</u></td></tr>
<tr><th id="589">589</th><td><u>    xmm5 = _mm_unpackhi_epi8(xmm5, xmm1);           \</u></td></tr>
<tr><th id="590">590</th><td><u>    xmm7 = _mm_slli_epi16(xmm7, 3);                 \</u></td></tr>
<tr><th id="591">591</th><td><u>    xmm5 = _mm_or_si128(xmm5, xmm7);                \</u></td></tr>
<tr><th id="592">592</th><td><u>    _mm_stream_si128((__m128i*)(p_buffer+8), xmm5);</u></td></tr>
<tr><th id="593">593</th><td></td></tr>
<tr><th id="594">594</th><td><u>#define SSE2_UNPACK_16_UNALIGNED                    \</u></td></tr>
<tr><th id="595">595</th><td><u>    xmm5 = _mm_set1_epi32(0xf8f8f8f8UL);            \</u></td></tr>
<tr><th id="596">596</th><td><u>    xmm0 = _mm_and_si128(xmm0, xmm5);               \</u></td></tr>
<tr><th id="597">597</th><td><u>    xmm1 = _mm_and_si128(xmm1, xmm5);               \</u></td></tr>
<tr><th id="598">598</th><td><u>    xmm5 = _mm_set1_epi32(0xfcfcfcfcUL);            \</u></td></tr>
<tr><th id="599">599</th><td><u>    xmm2 = _mm_and_si128(xmm2, xmm5);               \</u></td></tr>
<tr><th id="600">600</th><td><u>    xmm0 = _mm_srli_epi16(xmm0, 3);                 \</u></td></tr>
<tr><th id="601">601</th><td><u>    xmm4 = _mm_setzero_si128();                     \</u></td></tr>
<tr><th id="602">602</th><td><u>    xmm5 = xmm0;                                    \</u></td></tr>
<tr><th id="603">603</th><td><u>    xmm7 = xmm2;                                    \</u></td></tr>
<tr><th id="604">604</th><td><u>    \</u></td></tr>
<tr><th id="605">605</th><td><u>    xmm2 = _mm_unpacklo_epi8(xmm2, xmm4);           \</u></td></tr>
<tr><th id="606">606</th><td><u>    xmm0 = _mm_unpacklo_epi8(xmm0, xmm1);           \</u></td></tr>
<tr><th id="607">607</th><td><u>    xmm2 = _mm_slli_epi16(xmm2, 3);                 \</u></td></tr>
<tr><th id="608">608</th><td><u>    xmm0 = _mm_or_si128(xmm0, xmm2);                \</u></td></tr>
<tr><th id="609">609</th><td><u>    _mm_storeu_si128((__m128i*)p_buffer, xmm0);     \</u></td></tr>
<tr><th id="610">610</th><td><u>    \</u></td></tr>
<tr><th id="611">611</th><td><u>    xmm7 = _mm_unpackhi_epi8(xmm7, xmm4);           \</u></td></tr>
<tr><th id="612">612</th><td><u>    xmm5 = _mm_unpackhi_epi8(xmm5, xmm1);           \</u></td></tr>
<tr><th id="613">613</th><td><u>    xmm7 = _mm_slli_epi16(xmm7, 3);                 \</u></td></tr>
<tr><th id="614">614</th><td><u>    xmm5 = _mm_or_si128(xmm5, xmm7);                \</u></td></tr>
<tr><th id="615">615</th><td><u>    _mm_storeu_si128((__m128i*)(p_buffer+8), xmm5);</u></td></tr>
<tr><th id="616">616</th><td></td></tr>
<tr><th id="617">617</th><td><u>#define SSE2_UNPACK_32_ARGB_ALIGNED                 \</u></td></tr>
<tr><th id="618">618</th><td><u>    xmm3 = _mm_setzero_si128();                     \</u></td></tr>
<tr><th id="619">619</th><td><u>    xmm4 = xmm0;                                    \</u></td></tr>
<tr><th id="620">620</th><td><u>    xmm4 = _mm_unpacklo_epi8(xmm4, xmm2);           \</u></td></tr>
<tr><th id="621">621</th><td><u>    xmm5 = xmm1;                                    \</u></td></tr>
<tr><th id="622">622</th><td><u>    xmm5 = _mm_unpacklo_epi8(xmm5, xmm3);           \</u></td></tr>
<tr><th id="623">623</th><td><u>    xmm6 = xmm4;                                    \</u></td></tr>
<tr><th id="624">624</th><td><u>    xmm4 = _mm_unpacklo_epi16(xmm4, xmm5);          \</u></td></tr>
<tr><th id="625">625</th><td><u>    _mm_stream_si128((__m128i*)(p_buffer), xmm4);   \</u></td></tr>
<tr><th id="626">626</th><td><u>    xmm6 = _mm_unpackhi_epi16(xmm6, xmm5);          \</u></td></tr>
<tr><th id="627">627</th><td><u>    _mm_stream_si128((__m128i*)(p_buffer+4), xmm6); \</u></td></tr>
<tr><th id="628">628</th><td><u>    xmm0 = _mm_unpackhi_epi8(xmm0, xmm2);           \</u></td></tr>
<tr><th id="629">629</th><td><u>    xmm1 = _mm_unpackhi_epi8(xmm1, xmm3);           \</u></td></tr>
<tr><th id="630">630</th><td><u>    xmm5 = xmm0;                                    \</u></td></tr>
<tr><th id="631">631</th><td><u>    xmm5 = _mm_unpacklo_epi16(xmm5, xmm1);          \</u></td></tr>
<tr><th id="632">632</th><td><u>    _mm_stream_si128((__m128i*)(p_buffer+8), xmm5); \</u></td></tr>
<tr><th id="633">633</th><td><u>    xmm0 = _mm_unpackhi_epi16(xmm0, xmm1);          \</u></td></tr>
<tr><th id="634">634</th><td><u>    _mm_stream_si128((__m128i*)(p_buffer+12), xmm0);</u></td></tr>
<tr><th id="635">635</th><td></td></tr>
<tr><th id="636">636</th><td><u>#define SSE2_UNPACK_32_ARGB_UNALIGNED               \</u></td></tr>
<tr><th id="637">637</th><td><u>    xmm3 = _mm_setzero_si128();                     \</u></td></tr>
<tr><th id="638">638</th><td><u>    xmm4 = xmm0;                                    \</u></td></tr>
<tr><th id="639">639</th><td><u>    xmm4 = _mm_unpacklo_epi8(xmm4, xmm2);           \</u></td></tr>
<tr><th id="640">640</th><td><u>    xmm5 = xmm1;                                    \</u></td></tr>
<tr><th id="641">641</th><td><u>    xmm5 = _mm_unpacklo_epi8(xmm5, xmm3);           \</u></td></tr>
<tr><th id="642">642</th><td><u>    xmm6 = xmm4;                                    \</u></td></tr>
<tr><th id="643">643</th><td><u>    xmm4 = _mm_unpacklo_epi16(xmm4, xmm5);          \</u></td></tr>
<tr><th id="644">644</th><td><u>    _mm_storeu_si128((__m128i*)(p_buffer), xmm4);   \</u></td></tr>
<tr><th id="645">645</th><td><u>    xmm6 = _mm_unpackhi_epi16(xmm6, xmm5);          \</u></td></tr>
<tr><th id="646">646</th><td><u>    _mm_storeu_si128((__m128i*)(p_buffer+4), xmm6); \</u></td></tr>
<tr><th id="647">647</th><td><u>    xmm0 = _mm_unpackhi_epi8(xmm0, xmm2);           \</u></td></tr>
<tr><th id="648">648</th><td><u>    xmm1 = _mm_unpackhi_epi8(xmm1, xmm3);           \</u></td></tr>
<tr><th id="649">649</th><td><u>    xmm5 = xmm0;                                    \</u></td></tr>
<tr><th id="650">650</th><td><u>    xmm5 = _mm_unpacklo_epi16(xmm5, xmm1);          \</u></td></tr>
<tr><th id="651">651</th><td><u>    _mm_storeu_si128((__m128i*)(p_buffer+8), xmm5); \</u></td></tr>
<tr><th id="652">652</th><td><u>    xmm0 = _mm_unpackhi_epi16(xmm0, xmm1);          \</u></td></tr>
<tr><th id="653">653</th><td><u>    _mm_storeu_si128((__m128i*)(p_buffer+12), xmm0);</u></td></tr>
<tr><th id="654">654</th><td></td></tr>
<tr><th id="655">655</th><td><u>#define SSE2_UNPACK_32_RGBA_ALIGNED                 \</u></td></tr>
<tr><th id="656">656</th><td><u>    xmm3 = _mm_setzero_si128();                     \</u></td></tr>
<tr><th id="657">657</th><td><u>    xmm4 = xmm2;                                    \</u></td></tr>
<tr><th id="658">658</th><td><u>    xmm4 = _mm_unpacklo_epi8(xmm4, xmm1);           \</u></td></tr>
<tr><th id="659">659</th><td><u>    xmm3 = _mm_unpacklo_epi8(xmm3, xmm0);           \</u></td></tr>
<tr><th id="660">660</th><td><u>    xmm5 = xmm3;                                    \</u></td></tr>
<tr><th id="661">661</th><td><u>    xmm3 = _mm_unpacklo_epi16(xmm3, xmm4);          \</u></td></tr>
<tr><th id="662">662</th><td><u>    _mm_stream_si128((__m128i*)(p_buffer), xmm3);   \</u></td></tr>
<tr><th id="663">663</th><td><u>    xmm5 = _mm_unpackhi_epi16(xmm5, xmm4);          \</u></td></tr>
<tr><th id="664">664</th><td><u>    _mm_stream_si128((__m128i*)(p_buffer+4), xmm5); \</u></td></tr>
<tr><th id="665">665</th><td><u>    xmm6 = _mm_setzero_si128();                     \</u></td></tr>
<tr><th id="666">666</th><td><u>    xmm2 = _mm_unpackhi_epi8(xmm2, xmm1);           \</u></td></tr>
<tr><th id="667">667</th><td><u>    xmm6 = _mm_unpackhi_epi8(xmm6, xmm0);           \</u></td></tr>
<tr><th id="668">668</th><td><u>    xmm0 = xmm6;                                    \</u></td></tr>
<tr><th id="669">669</th><td><u>    xmm6 = _mm_unpacklo_epi16(xmm6, xmm2);          \</u></td></tr>
<tr><th id="670">670</th><td><u>    _mm_stream_si128((__m128i*)(p_buffer+8), xmm6); \</u></td></tr>
<tr><th id="671">671</th><td><u>    xmm0 = _mm_unpackhi_epi16(xmm0, xmm2);          \</u></td></tr>
<tr><th id="672">672</th><td><u>    _mm_stream_si128((__m128i*)(p_buffer+12), xmm0);</u></td></tr>
<tr><th id="673">673</th><td></td></tr>
<tr><th id="674">674</th><td><u>#define SSE2_UNPACK_32_RGBA_UNALIGNED               \</u></td></tr>
<tr><th id="675">675</th><td><u>    xmm3 = _mm_setzero_si128();                     \</u></td></tr>
<tr><th id="676">676</th><td><u>    xmm4 = xmm2;                                    \</u></td></tr>
<tr><th id="677">677</th><td><u>    xmm4 = _mm_unpacklo_epi8(xmm4, xmm1);           \</u></td></tr>
<tr><th id="678">678</th><td><u>    xmm3 = _mm_unpacklo_epi8(xmm3, xmm0);           \</u></td></tr>
<tr><th id="679">679</th><td><u>    xmm5 = xmm3;                                    \</u></td></tr>
<tr><th id="680">680</th><td><u>    xmm3 = _mm_unpacklo_epi16(xmm3, xmm4);          \</u></td></tr>
<tr><th id="681">681</th><td><u>    _mm_storeu_si128((__m128i*)(p_buffer), xmm3);   \</u></td></tr>
<tr><th id="682">682</th><td><u>    xmm5 = _mm_unpackhi_epi16(xmm5, xmm4);          \</u></td></tr>
<tr><th id="683">683</th><td><u>    _mm_storeu_si128((__m128i*)(p_buffer+4), xmm5); \</u></td></tr>
<tr><th id="684">684</th><td><u>    xmm6 = _mm_setzero_si128();                     \</u></td></tr>
<tr><th id="685">685</th><td><u>    xmm2 = _mm_unpackhi_epi8(xmm2, xmm1);           \</u></td></tr>
<tr><th id="686">686</th><td><u>    xmm6 = _mm_unpackhi_epi8(xmm6, xmm0);           \</u></td></tr>
<tr><th id="687">687</th><td><u>    xmm0 = xmm6;                                    \</u></td></tr>
<tr><th id="688">688</th><td><u>    xmm6 = _mm_unpacklo_epi16(xmm6, xmm2);          \</u></td></tr>
<tr><th id="689">689</th><td><u>    _mm_storeu_si128((__m128i*)(p_buffer+8), xmm6); \</u></td></tr>
<tr><th id="690">690</th><td><u>    xmm0 = _mm_unpackhi_epi16(xmm0, xmm2);          \</u></td></tr>
<tr><th id="691">691</th><td><u>    _mm_storeu_si128((__m128i*)(p_buffer+12), xmm0);</u></td></tr>
<tr><th id="692">692</th><td></td></tr>
<tr><th id="693">693</th><td><u>#define SSE2_UNPACK_32_BGRA_ALIGNED                 \</u></td></tr>
<tr><th id="694">694</th><td><u>    xmm3 = _mm_setzero_si128();                     \</u></td></tr>
<tr><th id="695">695</th><td><u>    xmm4 = xmm2;                                    \</u></td></tr>
<tr><th id="696">696</th><td><u>    xmm4 = _mm_unpacklo_epi8(xmm4, xmm0);           \</u></td></tr>
<tr><th id="697">697</th><td><u>    xmm3 = _mm_unpacklo_epi8(xmm3, xmm1);           \</u></td></tr>
<tr><th id="698">698</th><td><u>    xmm5 = xmm3;                                    \</u></td></tr>
<tr><th id="699">699</th><td><u>    xmm3 = _mm_unpacklo_epi16(xmm3, xmm4);          \</u></td></tr>
<tr><th id="700">700</th><td><u>    _mm_stream_si128((__m128i*)(p_buffer), xmm3);   \</u></td></tr>
<tr><th id="701">701</th><td><u>    xmm5 = _mm_unpackhi_epi16(xmm5, xmm4);          \</u></td></tr>
<tr><th id="702">702</th><td><u>    _mm_stream_si128((__m128i*)(p_buffer+4), xmm5); \</u></td></tr>
<tr><th id="703">703</th><td><u>    xmm6 = _mm_setzero_si128();                     \</u></td></tr>
<tr><th id="704">704</th><td><u>    xmm2 = _mm_unpackhi_epi8(xmm2, xmm0);           \</u></td></tr>
<tr><th id="705">705</th><td><u>    xmm6 = _mm_unpackhi_epi8(xmm6, xmm1);           \</u></td></tr>
<tr><th id="706">706</th><td><u>    xmm0 = xmm6;                                    \</u></td></tr>
<tr><th id="707">707</th><td><u>    xmm6 = _mm_unpacklo_epi16(xmm6, xmm2);          \</u></td></tr>
<tr><th id="708">708</th><td><u>    _mm_stream_si128((__m128i*)(p_buffer+8), xmm6); \</u></td></tr>
<tr><th id="709">709</th><td><u>    xmm0 = _mm_unpackhi_epi16(xmm0, xmm2);          \</u></td></tr>
<tr><th id="710">710</th><td><u>    _mm_stream_si128((__m128i*)(p_buffer+12), xmm0);</u></td></tr>
<tr><th id="711">711</th><td></td></tr>
<tr><th id="712">712</th><td><u>#define SSE2_UNPACK_32_BGRA_UNALIGNED               \</u></td></tr>
<tr><th id="713">713</th><td><u>    xmm3 = _mm_setzero_si128();                     \</u></td></tr>
<tr><th id="714">714</th><td><u>    xmm4 = xmm2;                                    \</u></td></tr>
<tr><th id="715">715</th><td><u>    xmm4 = _mm_unpacklo_epi8(xmm4, xmm0);           \</u></td></tr>
<tr><th id="716">716</th><td><u>    xmm3 = _mm_unpacklo_epi8(xmm3, xmm1);           \</u></td></tr>
<tr><th id="717">717</th><td><u>    xmm5 = xmm3;                                    \</u></td></tr>
<tr><th id="718">718</th><td><u>    xmm3 = _mm_unpacklo_epi16(xmm3, xmm4);          \</u></td></tr>
<tr><th id="719">719</th><td><u>    _mm_storeu_si128((__m128i*)(p_buffer), xmm3);   \</u></td></tr>
<tr><th id="720">720</th><td><u>    xmm5 = _mm_unpackhi_epi16(xmm5, xmm4);          \</u></td></tr>
<tr><th id="721">721</th><td><u>    _mm_storeu_si128((__m128i*)(p_buffer+4), xmm5); \</u></td></tr>
<tr><th id="722">722</th><td><u>    xmm6 = _mm_setzero_si128();                     \</u></td></tr>
<tr><th id="723">723</th><td><u>    xmm2 = _mm_unpackhi_epi8(xmm2, xmm0);           \</u></td></tr>
<tr><th id="724">724</th><td><u>    xmm6 = _mm_unpackhi_epi8(xmm6, xmm1);           \</u></td></tr>
<tr><th id="725">725</th><td><u>    xmm0 = xmm6;                                    \</u></td></tr>
<tr><th id="726">726</th><td><u>    xmm6 = _mm_unpacklo_epi16(xmm6, xmm2);          \</u></td></tr>
<tr><th id="727">727</th><td><u>    _mm_storeu_si128((__m128i*)(p_buffer+8), xmm6); \</u></td></tr>
<tr><th id="728">728</th><td><u>    xmm0 = _mm_unpackhi_epi16(xmm0, xmm2);          \</u></td></tr>
<tr><th id="729">729</th><td><u>    _mm_storeu_si128((__m128i*)(p_buffer+12), xmm0);</u></td></tr>
<tr><th id="730">730</th><td></td></tr>
<tr><th id="731">731</th><td><u>#define SSE2_UNPACK_32_ABGR_ALIGNED                 \</u></td></tr>
<tr><th id="732">732</th><td><u>    xmm3 = _mm_setzero_si128();                     \</u></td></tr>
<tr><th id="733">733</th><td><u>    xmm4 = xmm1;                                    \</u></td></tr>
<tr><th id="734">734</th><td><u>    xmm4 = _mm_unpacklo_epi8(xmm4, xmm2);           \</u></td></tr>
<tr><th id="735">735</th><td><u>    xmm5 = xmm0;                                    \</u></td></tr>
<tr><th id="736">736</th><td><u>    xmm5 = _mm_unpacklo_epi8(xmm5, xmm3);           \</u></td></tr>
<tr><th id="737">737</th><td><u>    xmm6 = xmm4;                                    \</u></td></tr>
<tr><th id="738">738</th><td><u>    xmm4 = _mm_unpacklo_epi16(xmm4, xmm5);          \</u></td></tr>
<tr><th id="739">739</th><td><u>    _mm_stream_si128((__m128i*)(p_buffer), xmm4);   \</u></td></tr>
<tr><th id="740">740</th><td><u>    xmm6 = _mm_unpackhi_epi16(xmm6, xmm5);          \</u></td></tr>
<tr><th id="741">741</th><td><u>    _mm_stream_si128((__m128i*)(p_buffer+4), xmm6); \</u></td></tr>
<tr><th id="742">742</th><td><u>    xmm1 = _mm_unpackhi_epi8(xmm1, xmm2);           \</u></td></tr>
<tr><th id="743">743</th><td><u>    xmm0 = _mm_unpackhi_epi8(xmm0, xmm3);           \</u></td></tr>
<tr><th id="744">744</th><td><u>    xmm2 = xmm1;                                    \</u></td></tr>
<tr><th id="745">745</th><td><u>    xmm1 = _mm_unpacklo_epi16(xmm1, xmm0);          \</u></td></tr>
<tr><th id="746">746</th><td><u>    _mm_stream_si128((__m128i*)(p_buffer+8), xmm1); \</u></td></tr>
<tr><th id="747">747</th><td><u>    xmm2 = _mm_unpackhi_epi16(xmm2, xmm0);          \</u></td></tr>
<tr><th id="748">748</th><td><u>    _mm_stream_si128((__m128i*)(p_buffer+12), xmm2);</u></td></tr>
<tr><th id="749">749</th><td></td></tr>
<tr><th id="750">750</th><td><u>#define SSE2_UNPACK_32_ABGR_UNALIGNED               \</u></td></tr>
<tr><th id="751">751</th><td><u>    xmm3 = _mm_setzero_si128();                     \</u></td></tr>
<tr><th id="752">752</th><td><u>    xmm4 = xmm1;                                    \</u></td></tr>
<tr><th id="753">753</th><td><u>    xmm4 = _mm_unpacklo_epi8(xmm4, xmm2);           \</u></td></tr>
<tr><th id="754">754</th><td><u>    xmm5 = xmm0;                                    \</u></td></tr>
<tr><th id="755">755</th><td><u>    xmm5 = _mm_unpacklo_epi8(xmm5, xmm3);           \</u></td></tr>
<tr><th id="756">756</th><td><u>    xmm6 = xmm4;                                    \</u></td></tr>
<tr><th id="757">757</th><td><u>    xmm4 = _mm_unpacklo_epi16(xmm4, xmm5);          \</u></td></tr>
<tr><th id="758">758</th><td><u>    _mm_storeu_si128((__m128i*)(p_buffer), xmm4);   \</u></td></tr>
<tr><th id="759">759</th><td><u>    xmm6 = _mm_unpackhi_epi16(xmm6, xmm5);          \</u></td></tr>
<tr><th id="760">760</th><td><u>    _mm_storeu_si128((__m128i*)(p_buffer+4), xmm6); \</u></td></tr>
<tr><th id="761">761</th><td><u>    xmm1 = _mm_unpackhi_epi8(xmm1, xmm2);           \</u></td></tr>
<tr><th id="762">762</th><td><u>    xmm0 = _mm_unpackhi_epi8(xmm0, xmm3);           \</u></td></tr>
<tr><th id="763">763</th><td><u>    xmm2 = xmm1;                                    \</u></td></tr>
<tr><th id="764">764</th><td><u>    xmm1 = _mm_unpacklo_epi16(xmm1, xmm0);          \</u></td></tr>
<tr><th id="765">765</th><td><u>    _mm_storeu_si128((__m128i*)(p_buffer+8), xmm1); \</u></td></tr>
<tr><th id="766">766</th><td><u>    xmm2 = _mm_unpackhi_epi16(xmm2, xmm0);          \</u></td></tr>
<tr><th id="767">767</th><td><u>    _mm_storeu_si128((__m128i*)(p_buffer+12), xmm2);</u></td></tr>
<tr><th id="768">768</th><td></td></tr>
<tr><th id="769">769</th><td><u>#<span data-ppcond="22">endif</span></u></td></tr>
<tr><th id="770">770</th><td></td></tr>
</table><hr/><p id='footer'>
Generated while processing <a href='i420_rgb16_x86.c.html'>vlc/modules/video_chroma/i420_rgb16_x86.c</a><br/>Generated on <em>2016-Oct-25</em> from project vlc revision <em>2.2.0-git</em><br />Powered by <a href='https://woboq.com'><img alt='Woboq' src='https://code.woboq.org/woboq-16.png' width='41' height='16' /></a> <a href='https://code.woboq.org'>Code Browser</a> 2.0.1
<br/>Generator usage only permitted with license.</p>
</div></body></html>
